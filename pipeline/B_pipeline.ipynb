{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Ditchley S2DS project August 2020 - Pipeline B<h1>\n",
    "    <h2>Team: Adam Hawken, Luca Lamoni, Elizabeth Nicholson, Robert Webster<h2>\n",
    "        \n",
    "This notebook (B_pipeline) will be dedicated to:\n",
    "- B1: Working directory and keyword setting\n",
    "- B2: Selection of inliers based on distribution of friends and followers (optional)       \n",
    "- B3: Download equal amounts of journalists and friends tweets and clean them\n",
    "- B4: Calculate H-Index for each user"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Section B1: Working directory and keyword setting<h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data directory & sub-directories already exist, skipping.\n"
     ]
    }
   ],
   "source": [
    "# Import modules and set up working directory\n",
    "import sys\n",
    "import os\n",
    "import time\n",
    "import logging\n",
    "import json\n",
    "import csv\n",
    "import threading\n",
    "import queue\n",
    "import asyncio \n",
    "import nest_asyncio\n",
    "nest_asyncio.apply()\n",
    "import twint\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "# Set up working directory\n",
    "# The working directory should reflect the structure of the Github repository https://github.com/S2DSLondon/Aug20_Ditchley\n",
    "sys.path.insert(1, 'C:/Users/Luca/Aug20_Ditchley/')\n",
    "from src.data import pipeline_setup\n",
    "pipeline_setup.build_data_dir('C:/Users/Luca/Aug20_Ditchley/')\n",
    "\n",
    "#Set again the keyword of interest\n",
    "keyword = 'cybersecurity'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Section B2: Selection of inliers based on distribution of friends and followers (optional)<h3>\n",
    "If interested in downloading tweets for ALL friends of journalists please skip this step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load friends user info\n",
    "df_friends_profiles  = pd.read_csv('../data/processed/'+keyword+'_user_friends_profiles.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>screen_name</th>\n",
       "      <th>name</th>\n",
       "      <th>location</th>\n",
       "      <th>user_description</th>\n",
       "      <th>user_friends_n</th>\n",
       "      <th>user_followers_n</th>\n",
       "      <th>prof_created_at</th>\n",
       "      <th>favourites_count</th>\n",
       "      <th>verified</th>\n",
       "      <th>statuses_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>335773502</td>\n",
       "      <td>_lucyingham</td>\n",
       "      <td>Lucy Ingham</td>\n",
       "      <td>London</td>\n",
       "      <td>editor of and digital magazines verdict magazi...</td>\n",
       "      <td>516</td>\n",
       "      <td>646</td>\n",
       "      <td>2011-07-15 06:29:08</td>\n",
       "      <td>2214</td>\n",
       "      <td>False</td>\n",
       "      <td>456</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>964233746865119233</td>\n",
       "      <td>jesscahaworth</td>\n",
       "      <td>Jessica Haworth</td>\n",
       "      <td>NaN</td>\n",
       "      <td>cybersecurity journalist at music buff and ski...</td>\n",
       "      <td>970</td>\n",
       "      <td>668</td>\n",
       "      <td>2018-02-15 20:23:34</td>\n",
       "      <td>459</td>\n",
       "      <td>False</td>\n",
       "      <td>583</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1186245031507693574</td>\n",
       "      <td>ad_nauseum74</td>\n",
       "      <td>Adam Bannister</td>\n",
       "      <td>NaN</td>\n",
       "      <td>journalist the daily swig cybersecurity</td>\n",
       "      <td>368</td>\n",
       "      <td>135</td>\n",
       "      <td>2019-10-21 11:38:12</td>\n",
       "      <td>114</td>\n",
       "      <td>False</td>\n",
       "      <td>277</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>602102154</td>\n",
       "      <td>j0ne_s_</td>\n",
       "      <td>üê¨üëΩ‚ò¢Ô∏èü§ó</td>\n",
       "      <td>Dublin / Mile End</td>\n",
       "      <td>labour party loser mars delight truther finder...</td>\n",
       "      <td>2508</td>\n",
       "      <td>1243</td>\n",
       "      <td>2012-06-07 16:47:41</td>\n",
       "      <td>24290</td>\n",
       "      <td>False</td>\n",
       "      <td>8786</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2851488433</td>\n",
       "      <td>maddiestone</td>\n",
       "      <td>Maddie Stone</td>\n",
       "      <td>NaN</td>\n",
       "      <td>security researcher project zero reverse engin...</td>\n",
       "      <td>997</td>\n",
       "      <td>32211</td>\n",
       "      <td>2014-10-11 03:33:11</td>\n",
       "      <td>11451</td>\n",
       "      <td>False</td>\n",
       "      <td>5098</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1712</th>\n",
       "      <td>519989509</td>\n",
       "      <td>mozillasecurity</td>\n",
       "      <td>Mozilla Security</td>\n",
       "      <td>NaN</td>\n",
       "      <td>official account of mozilla security</td>\n",
       "      <td>55</td>\n",
       "      <td>686</td>\n",
       "      <td>2012-03-10 01:36:23</td>\n",
       "      <td>23</td>\n",
       "      <td>False</td>\n",
       "      <td>68</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1713</th>\n",
       "      <td>3218740888</td>\n",
       "      <td>hollygraceful</td>\n",
       "      <td>Holly Graceful</td>\n",
       "      <td>Manchester, England</td>\n",
       "      <td>managing director interested in tech security ...</td>\n",
       "      <td>934</td>\n",
       "      <td>13297</td>\n",
       "      <td>2015-04-28 20:30:30</td>\n",
       "      <td>439</td>\n",
       "      <td>False</td>\n",
       "      <td>1284</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1714</th>\n",
       "      <td>804230797993443328</td>\n",
       "      <td>sudo_sudoka</td>\n",
       "      <td>Sudoka</td>\n",
       "      <td>The Matrix (Inside now)</td>\n",
       "      <td>threat analyst bounty hunter ctf player securi...</td>\n",
       "      <td>151</td>\n",
       "      <td>384</td>\n",
       "      <td>2016-12-01 07:48:58</td>\n",
       "      <td>968</td>\n",
       "      <td>False</td>\n",
       "      <td>268</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1715</th>\n",
       "      <td>836272957</td>\n",
       "      <td>nordvpn</td>\n",
       "      <td>NordVPN</td>\n",
       "      <td>Privacy solutions</td>\n",
       "      <td>everyone deserves a secure private and unrestr...</td>\n",
       "      <td>3514</td>\n",
       "      <td>77272</td>\n",
       "      <td>2012-09-20 20:07:18</td>\n",
       "      <td>2638</td>\n",
       "      <td>True</td>\n",
       "      <td>20011</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1716</th>\n",
       "      <td>543802336</td>\n",
       "      <td>torguard</td>\n",
       "      <td>TorGuard</td>\n",
       "      <td>NaN</td>\n",
       "      <td>anonymous vpn proxy and email services privacy...</td>\n",
       "      <td>2</td>\n",
       "      <td>9461</td>\n",
       "      <td>2012-04-02 23:07:17</td>\n",
       "      <td>178</td>\n",
       "      <td>False</td>\n",
       "      <td>1427</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1717 rows √ó 11 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                  user_id      screen_name              name  \\\n",
       "0               335773502      _lucyingham       Lucy Ingham   \n",
       "1      964233746865119233    jesscahaworth   Jessica Haworth   \n",
       "2     1186245031507693574     ad_nauseum74    Adam Bannister   \n",
       "3               602102154          j0ne_s_             üê¨üëΩ‚ò¢Ô∏èü§ó   \n",
       "4              2851488433      maddiestone      Maddie Stone   \n",
       "...                   ...              ...               ...   \n",
       "1712            519989509  mozillasecurity  Mozilla Security   \n",
       "1713           3218740888    hollygraceful    Holly Graceful   \n",
       "1714   804230797993443328      sudo_sudoka            Sudoka   \n",
       "1715            836272957          nordvpn           NordVPN   \n",
       "1716            543802336         torguard          TorGuard   \n",
       "\n",
       "                     location  \\\n",
       "0                      London   \n",
       "1                         NaN   \n",
       "2                         NaN   \n",
       "3           Dublin / Mile End   \n",
       "4                         NaN   \n",
       "...                       ...   \n",
       "1712                      NaN   \n",
       "1713      Manchester, England   \n",
       "1714  The Matrix (Inside now)   \n",
       "1715        Privacy solutions   \n",
       "1716                      NaN   \n",
       "\n",
       "                                       user_description  user_friends_n  \\\n",
       "0     editor of and digital magazines verdict magazi...             516   \n",
       "1     cybersecurity journalist at music buff and ski...             970   \n",
       "2               journalist the daily swig cybersecurity             368   \n",
       "3     labour party loser mars delight truther finder...            2508   \n",
       "4     security researcher project zero reverse engin...             997   \n",
       "...                                                 ...             ...   \n",
       "1712               official account of mozilla security              55   \n",
       "1713  managing director interested in tech security ...             934   \n",
       "1714  threat analyst bounty hunter ctf player securi...             151   \n",
       "1715  everyone deserves a secure private and unrestr...            3514   \n",
       "1716  anonymous vpn proxy and email services privacy...               2   \n",
       "\n",
       "      user_followers_n      prof_created_at  favourites_count  verified  \\\n",
       "0                  646  2011-07-15 06:29:08              2214     False   \n",
       "1                  668  2018-02-15 20:23:34               459     False   \n",
       "2                  135  2019-10-21 11:38:12               114     False   \n",
       "3                 1243  2012-06-07 16:47:41             24290     False   \n",
       "4                32211  2014-10-11 03:33:11             11451     False   \n",
       "...                ...                  ...               ...       ...   \n",
       "1712               686  2012-03-10 01:36:23                23     False   \n",
       "1713             13297  2015-04-28 20:30:30               439     False   \n",
       "1714               384  2016-12-01 07:48:58               968     False   \n",
       "1715             77272  2012-09-20 20:07:18              2638      True   \n",
       "1716              9461  2012-04-02 23:07:17               178     False   \n",
       "\n",
       "      statuses_count  \n",
       "0                456  \n",
       "1                583  \n",
       "2                277  \n",
       "3               8786  \n",
       "4               5098  \n",
       "...              ...  \n",
       "1712              68  \n",
       "1713            1284  \n",
       "1714             268  \n",
       "1715           20011  \n",
       "1716            1427  \n",
       "\n",
       "[1717 rows x 11 columns]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "user_profiles = pd.read_csv('../data/processed/'+keyword+'_user_profiles.csv' )\n",
    "user_friends_profiles = pd.read_csv('../data/processed/'+keyword+'_user_friends_profiles.csv' )\n",
    "users_df = pd.concat([user_profiles,user_friends_profiles])\n",
    "users_df = users_df.drop_duplicates().reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate chi2s\n",
    "from src.graph_database import graphdb as gdb\n",
    "#%pylab\n",
    "no_loners = gdb.get_chi2(users_df)\n",
    "\n",
    "inliers = no_loners[no_loners['chi2']<6.18]\n",
    "outliers = no_loners[no_loners['chi2']>6.18]\n",
    "\n",
    "from src import plots\n",
    "inliner_plot = plots.plot_user_inliers(inliers, outliers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1824"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "no_loners.to_csv('../data/processed/'+keyword+'_user_friends_profiles.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df_friends_subset = inliers\n",
    "df_friends_subset.to_csv('../data/processed/'+keyword+'_user_friends_profiles_subset.csv', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Section B3: Download equal amounts of journalists and friends tweets and clean them<h3>\n",
    "    <h4>B3.1: Tweets download using Twitter API<h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We do this step to dowload an equal number of tweets from all users (journalists and friends) in order to carry out the H-Index on the same amounts of tweets\n",
    "df_friends_handles = pd.read_csv('../data/processed/'+keyword+'_journalist_friends.csv')\n",
    "df_user_list_extended = pd.concat([pd.DataFrame(df_friends_handles.screen_name.unique()),df_friends_handles.friend]).drop_duplicates().reset_index(drop=True)\n",
    "df_user_list_extended.columns = ['user']\n",
    "users = df_user_list_extended.user.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load twitter API credentials and return a tweepy API instance\n",
    "import json\n",
    "import tweepy\n",
    "from src.data import api_tweepy as api\n",
    "\n",
    "# Input: path of json file with credentials / Output: tweepy.api.API\n",
    "tw_api = api.connect_API('../src/data/twitter_credentials.json')\n",
    "# Import fuctions\n",
    "from src.data.api_tweet_tools import request_user_timeline, batch_request_user_timeline\n",
    "\n",
    "# Define the destination folder in which the csv files will be saved\n",
    "dest_dir = '../data/raw/friends_tweets/'\n",
    "# If the folder does not exist, this will create it\n",
    "if not os.path.exists(dest_dir):\n",
    "    os.makedirs(dest_dir)\n",
    "\n",
    "# Finally, dowload the tweets for all the jornalists friends. \n",
    "# Input: tw_api = tweepy.api.API, list = list of friends from df, filepath = destination folder set above, \n",
    "# api_delay = seconds of delay between requests |note: if you reach 900 requests in less that 15 min the function will pause and \n",
    "# then resume when the 15 min have passed|, n_tweets = max 200 for a single request\n",
    "batch_request_user_timeline(tw_api, users, filepath = dest_dir, api_delay = 0.2, n_tweets=200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>B3.2: Tweet data cleaning<h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean the tweet dataframes\n",
    "from src.data import data_cleanup as dc\n",
    "import os\n",
    "from tqdm.auto import tqdm\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Define source and destination folders for the cleaning function\n",
    "src_dir = '../data/raw/friends_tweets/'\n",
    "dest_dir = '../data/cleaned/friends_tweets/'\n",
    "# If the folder does not exist, this will create it\n",
    "if not os.path.exists(dest_dir):\n",
    "    os.makedirs(dest_dir)\n",
    "\n",
    "# Loop through the files, load 1 csv at the time, clean it, and save it to destination folder\n",
    "files = [file for file in os.listdir(src_dir) if os.path.isfile(os.path.join(src_dir, file))]\n",
    "with tqdm(total=len(files), desc='Files') as pbar:\n",
    "    for file in files:\n",
    "        raw_df = pd.read_csv(os.path.join(src_dir, file), low_memory=False)\n",
    "        cleaned_df = dc.clean_API_dataframe(raw_df)\n",
    "        cleaned_df.to_csv(os.path.join(dest_dir, file), index=False)\n",
    "        pbar.update(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Section B4: Calculate H-Index<h3>\n",
    "    <h4>B4.1: H-Index function<h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import modules and functions\n",
    "from src.data import H_Index_tools as h_tools\n",
    "import os\n",
    "from tqdm.auto import tqdm\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Define source and destination folders for the H_index function\n",
    "src_dir = '../data/cleaned/friends_tweets/'\n",
    "dest_dir = '../data/processed/'\n",
    "# This function will loop throught the journalist friends tweet csv files and calculate H-Index and the sum of retweets&likes for each friend\n",
    "h_tools.loop_csv_H_index(src_dir,dest_dir, keyword)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>B3.2: Merge the results of H-Index with exisiting journalist friends profiles<h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import journalist user profiles, friends user profiles and concatenate them\n",
    "\n",
    "\n",
    "\n",
    "# Join the dataframe of all users (journalists + friends) with the sums of their likes, retweets and thier H-Index\n",
    " # Import\n",
    "like_rt_count_users = pd.read_csv('../data/processed/'+keyword+'_like_rt_count_users.csv' )\n",
    "h_index_users = pd.read_csv('../data/processed/'+keyword+'_h_index_users.csv' )\n",
    "# Merge 1\n",
    "df_user_profiles_metrics = pd.merge(user_friends_profiles, like_rt_count_users, how='inner', on='screen_name')\n",
    "# Merge 2\n",
    "df_users_profiles_metrics = pd.merge(df_user_profiles_metrics, h_index_users, how='inner', on='screen_name')\n",
    "# Drop duplicates (the journalists rows sometimes are repeated)\n",
    "df_users_profiles_metrics.drop_duplicates(subset ='screen_name',keep = 'first', inplace = True)\n",
    "# Save final df\n",
    "df_users_profiles_metrics.to_csv('../data/processed/'+keyword+'_friends_profiles_metrics.csv', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>B3.3: Plot the results of H-Index<h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.data import H_Index_tools as h_tools\n",
    "h_tools.plot_H_index(df_users_profiles_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
