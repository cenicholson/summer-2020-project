{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ditchley S2DS project August 2020 - Pipeline C\n",
    "\n",
    "## Team: Adam Hawken, Luca Lamoni, Elizabeth Nicholson, Robert Webster\n",
    "\n",
    "This notebook (C_pipeline) will be dedicated to:\n",
    "C1: Package and data imports\n",
    "C2: Hashtag analysis, topic assignment and searching for keywords\n",
    "C3: Preprocessing for training Doc2Vec model\n",
    "C4: Training Doc2Vec model and testing robustness\n",
    "C5: Apply Doc2Vec to tweets and some basic analysis\n",
    "\n",
    "\n",
    "## Section C1: Package and data imports"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### C1.1: Package intallations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# installations an imports\n",
    "\n",
    "#!pip install gensim==3.8.3\n",
    "#!pip install wordcloud\n",
    "\n",
    "import sys\n",
    "import os\n",
    "import itertools as it\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "tqdm.pandas(desc=\"progress-bar\")\n",
    "from gensim.models import Doc2Vec\n",
    "from sklearn import utils\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import decomposition as skd\n",
    "from sklearn import feature_extraction as skfe\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "import gensim\n",
    "from gensim.models.doc2vec import TaggedDocument\n",
    "import re\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "from collections import Counter\n",
    "import difflib\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from PIL import Image\n",
    "from wordcloud import WordCloud, STOPWORDS, ImageColorGenerator\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set path for calling functions\n",
    "sys.path.insert(1, '../')\n",
    "\n",
    "from src.topics import hashtag_analysis as ha\n",
    "#import src.plots\n",
    "from src.topics import preprocessing as pp\n",
    "from src.topics import topic_modelling as tm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### C1.2: importing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Files: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 40/40 [00:53<00:00,  1.34s/it]\n"
     ]
    }
   ],
   "source": [
    "# import data\n",
    "\n",
    "src_dir = r'C:\\Users\\elizabeth\\Documents\\cleaned tweets\\cyber friends tweets'\n",
    "\n",
    "files = [file for file in os.listdir(src_dir) if os.path.isfile(os.path.join(src_dir, file))] # build list of files to iterate through\n",
    "\n",
    "data = pd.DataFrame()\n",
    "\n",
    "with tqdm(total=len(files), desc='Files') as pbar:\n",
    "    for file in files:\n",
    "        df = pd.read_csv(os.path.join(src_dir, file), low_memory=False)\n",
    "        data = pd.concat([data, df[['tweet_id', 'text', 'hashtags']]], axis=0)\n",
    "        pbar.update(1)\n",
    "        \n",
    "data = data[data['hashtags'].astype(str) != '[]'] # remove empties\n",
    "data['text'] = data['text'].apply(lambda x: x.lower() if isinstance(x, str) else '')\n",
    "data = data[data.text != '']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section C2: Hashtag analysis, topic assignment and keyword searches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### C2.1 Hashtag analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 722766 entries, 1 to 87447\n",
      "Columns: 217116 entries, 00050c to ùôèùôêùôÄùôéùòøùòºùôîùôèùôêùôâùôÄùôé\n",
      "dtypes: Sparse[int64, 0](217116)\n",
      "memory usage: 21.7 MB\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "9.012709550464738e-06"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# vectorize the hashtags\n",
    "\n",
    "hashtag_observations = ha.vectorize_wordlists(data['hashtags'])\n",
    "hashtag_observations.info()\n",
    "hashtag_observations.sparse.density"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sum of the explained variance = 0.189.\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Use TruncatedSVD on vectorised documents as dimensionality reduction to generate topics.\n",
    "\n",
    "hashtag_dimred, svd_comps = ha.SVD_on_vectors(hashtag_observations, n_components=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>keywords</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>topic</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[covid19]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[ai, cybersecurity, fintech]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[cybersecurity, infosec]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[fintech]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[coronavirus]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>[infosec, iot, security, tech, technology]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>[bigdata, datascience, machinelearning]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>[infosec, security]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>[iot, privacy, security]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>[bitcoin, blockchain, infosec, iot]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>[banking, blockchain, data, innovation, paymen...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>[banking, blockchain, data, payments, tech]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>[iot]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>[brexit, london]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>[innovation, security]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>[brexit]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>[innovation, privacy]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>[data, startup, startups]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>[china, covid, payments, startup, startups]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>[china, covid, data]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>[covid]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>[blacklivesmatter]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>[5g, artificialintelligence, data, payments]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>[bitcoin, payments]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>[5g, bitcoin, ransomware]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>[malware, ransomware]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>[cannabis, hemp, hempology, hightimes, homegro...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>[cyber, ransomware]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>[5g, ransomware]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>[startup]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>[digital, digitaltransformation]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>[health, mentalhealth, podcast]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>[environment, health, mentalhealth, research, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>[artificialintelligence, bigdata, datascience,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>[beirut, belarus, lebanon]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>[belarus]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>[bolivia, jobs, journalism, tourism, travel]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>[compliance, datascience, startups]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>[business]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>[apple, fcpx, film, filmmaking, finalcutprox, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>[digitaltransformation]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>[ar, vr]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>[property, realestate]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>[believeinyou, boys, breakthesilence, england,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>[gdpr, lockdown, malware]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>[lockdown]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>[bigdata, gdpr, insurance, insurtech]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>[jobs, media]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>[insurtech, uk]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>[insurance, insurtech]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                keywords\n",
       "topic                                                   \n",
       "0                                              [covid19]\n",
       "1                           [ai, cybersecurity, fintech]\n",
       "2                               [cybersecurity, infosec]\n",
       "3                                              [fintech]\n",
       "4                                          [coronavirus]\n",
       "5             [infosec, iot, security, tech, technology]\n",
       "6                [bigdata, datascience, machinelearning]\n",
       "7                                    [infosec, security]\n",
       "8                               [iot, privacy, security]\n",
       "9                    [bitcoin, blockchain, infosec, iot]\n",
       "10     [banking, blockchain, data, innovation, paymen...\n",
       "11           [banking, blockchain, data, payments, tech]\n",
       "12                                                 [iot]\n",
       "13                                      [brexit, london]\n",
       "14                                [innovation, security]\n",
       "15                                              [brexit]\n",
       "16                                 [innovation, privacy]\n",
       "17                             [data, startup, startups]\n",
       "18           [china, covid, payments, startup, startups]\n",
       "19                                  [china, covid, data]\n",
       "20                                               [covid]\n",
       "21                                    [blacklivesmatter]\n",
       "22          [5g, artificialintelligence, data, payments]\n",
       "23                                   [bitcoin, payments]\n",
       "24                             [5g, bitcoin, ransomware]\n",
       "25                                 [malware, ransomware]\n",
       "26     [cannabis, hemp, hempology, hightimes, homegro...\n",
       "27                                   [cyber, ransomware]\n",
       "28                                      [5g, ransomware]\n",
       "29                                             [startup]\n",
       "30                      [digital, digitaltransformation]\n",
       "31                       [health, mentalhealth, podcast]\n",
       "32     [environment, health, mentalhealth, research, ...\n",
       "33     [artificialintelligence, bigdata, datascience,...\n",
       "34                            [beirut, belarus, lebanon]\n",
       "35                                             [belarus]\n",
       "36          [bolivia, jobs, journalism, tourism, travel]\n",
       "37                   [compliance, datascience, startups]\n",
       "38                                            [business]\n",
       "39     [apple, fcpx, film, filmmaking, finalcutprox, ...\n",
       "40                               [digitaltransformation]\n",
       "41                                              [ar, vr]\n",
       "42                                [property, realestate]\n",
       "43     [believeinyou, boys, breakthesilence, england,...\n",
       "44                             [gdpr, lockdown, malware]\n",
       "45                                            [lockdown]\n",
       "46                 [bigdata, gdpr, insurance, insurtech]\n",
       "47                                         [jobs, media]\n",
       "48                                       [insurtech, uk]\n",
       "49                                [insurance, insurtech]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# make topic keywords\n",
    "\n",
    "topics = ha.make_topic_keywords_from_svd(svd_comps, threshold=0.2)\n",
    "topics.head(50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### C2.2 Topic assignment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define topics with some human input here is necessary \n",
    "\n",
    "topics = [['covid19', 'lockdown', 'coronavirus', 'covid', 'pandemic'], \n",
    "['ai', 'ml', 'datascience', 'artificalintelligence', 'data', 'bigdata'], \n",
    "['cybersecurity', 'iot', 'robotics', 'infosec', 'cyber'],\n",
    "['fintech', 'blockchain', 'payments', 'cryptocurrency', 'bitcoin', 'cyrpto'],\n",
    "['startup', 'startups'],\n",
    "['malware', 'ransomware'], \n",
    "['environment', 'health', 'mentalhealth', 'research', 'wellbeing'],\n",
    "['beirut', 'belarus', 'lebanon']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### C2.3 Searching for hashtags or keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>text</th>\n",
       "      <th>hashtags</th>\n",
       "      <th>topic_labels</th>\n",
       "      <th>main_label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1293934001514848256</td>\n",
       "      <td>rt breaking via fbi to join beirut blast probe...</td>\n",
       "      <td>['breaking', 'fbi', 'beirut', 'probe']</td>\n",
       "      <td>[7]</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1293580602055299072</td>\n",
       "      <td>rt breaking lebanon prosecutor to question sev...</td>\n",
       "      <td>['breaking', 'beirutblast']</td>\n",
       "      <td>[10]</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1292021027543953408</td>\n",
       "      <td>rt according to the lebanese health ministry o...</td>\n",
       "      <td>['beirutblast']</td>\n",
       "      <td>[10]</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1291401752911257606</td>\n",
       "      <td>rt breaking imf urges lebanon to break reform ...</td>\n",
       "      <td>['breaking', 'lebanon']</td>\n",
       "      <td>[7]</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>1291331749956509698</td>\n",
       "      <td>macron says will pitch new political deal to l...</td>\n",
       "      <td>['lebanon']</td>\n",
       "      <td>[7]</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87365</th>\n",
       "      <td>1270406159355334656</td>\n",
       "      <td>rt another fascinating panel this afternoon th...</td>\n",
       "      <td>['trust', 'technology']</td>\n",
       "      <td>[10]</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87379</th>\n",
       "      <td>1268578990220029953</td>\n",
       "      <td>rt join this cogx panel to hear the latest exp...</td>\n",
       "      <td>['cogx2020']</td>\n",
       "      <td>[10]</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87437</th>\n",
       "      <td>1260516465435906050</td>\n",
       "      <td>rt join us today bst to discuss whether a join...</td>\n",
       "      <td>['covid19uk', 'contacttracing']</td>\n",
       "      <td>[10]</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87439</th>\n",
       "      <td>1260119164502659072</td>\n",
       "      <td>rt could ai get us out of lockdown a group of ...</td>\n",
       "      <td>['ai']</td>\n",
       "      <td>[1]</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87447</th>\n",
       "      <td>1258725356988764162</td>\n",
       "      <td>rt view from on contacttracingapp carly kind d...</td>\n",
       "      <td>['contacttracingapp']</td>\n",
       "      <td>[10]</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>722766 rows √ó 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                  tweet_id                                               text  \\\n",
       "1      1293934001514848256  rt breaking via fbi to join beirut blast probe...   \n",
       "2      1293580602055299072  rt breaking lebanon prosecutor to question sev...   \n",
       "6      1292021027543953408  rt according to the lebanese health ministry o...   \n",
       "9      1291401752911257606  rt breaking imf urges lebanon to break reform ...   \n",
       "12     1291331749956509698  macron says will pitch new political deal to l...   \n",
       "...                    ...                                                ...   \n",
       "87365  1270406159355334656  rt another fascinating panel this afternoon th...   \n",
       "87379  1268578990220029953  rt join this cogx panel to hear the latest exp...   \n",
       "87437  1260516465435906050  rt join us today bst to discuss whether a join...   \n",
       "87439  1260119164502659072  rt could ai get us out of lockdown a group of ...   \n",
       "87447  1258725356988764162  rt view from on contacttracingapp carly kind d...   \n",
       "\n",
       "                                     hashtags topic_labels  main_label  \n",
       "1      ['breaking', 'fbi', 'beirut', 'probe']          [7]           7  \n",
       "2                 ['breaking', 'beirutblast']         [10]          10  \n",
       "6                             ['beirutblast']         [10]          10  \n",
       "9                     ['breaking', 'lebanon']          [7]           7  \n",
       "12                                ['lebanon']          [7]           7  \n",
       "...                                       ...          ...         ...  \n",
       "87365                 ['trust', 'technology']         [10]          10  \n",
       "87379                            ['cogx2020']         [10]          10  \n",
       "87437         ['covid19uk', 'contacttracing']         [10]          10  \n",
       "87439                                  ['ai']          [1]           1  \n",
       "87447                   ['contacttracingapp']         [10]          10  \n",
       "\n",
       "[722766 rows x 5 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# run this cell if you want to match the topic list with hashtags \n",
    "\n",
    "possible_topics = data['hashtags'].apply(lambda x: ha.check_for_matches(eval(x), topic_list=topics, number_other_topic=10))\n",
    "\n",
    "labelled_data = data.assign(topic_labels=possible_topics.values)\n",
    "labelled_data['main_label'] = labelled_data['topic_labels'].apply(lambda x: x[0])\n",
    "labelled_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "flat_topics = list(set([item for sublist in topics for item in sublist]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Files: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 40/40 [01:56<00:00,  2.90s/it]\n"
     ]
    }
   ],
   "source": [
    "# import data, this time finding matched for keywords\n",
    "\n",
    "src_dir = r'C:\\Users\\elizabeth\\Documents\\cleaned tweets\\cyber friends tweets'\n",
    "\n",
    "files = [file for file in os.listdir(src_dir) if os.path.isfile(os.path.join(src_dir, file))] # build list of files to iterate through\n",
    "\n",
    "data_new= pd.DataFrame()\n",
    "\n",
    "with tqdm(total=len(files), desc='Files') as pbar:\n",
    "    for file in files:\n",
    "        df = pd.read_csv(os.path.join(src_dir, file), low_memory=False)\n",
    "        df['text'] = df['text'].apply(lambda x: x.lower() if isinstance(x, str) else '')\n",
    "        df['keywords'] = df['text'].apply(lambda x: ha.check_keyword_matches(x, flat_topics))\n",
    "        df = df[df['keywords'].astype(str)!='[]']\n",
    "        data_new = pd.concat([data_new, df[['tweet_id', 'text', 'hashtags', 'keywords']]], axis=0, ignore_index=True)\n",
    "        pbar.update(1)\n",
    "        \n",
    "data_new = data_new[data_new.text != '']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>text</th>\n",
       "      <th>hashtags</th>\n",
       "      <th>topic_labels</th>\n",
       "      <th>main_label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1293934001514848256</td>\n",
       "      <td>rt breaking via fbi to join beirut blast probe...</td>\n",
       "      <td>['breaking', 'fbi', 'beirut', 'probe']</td>\n",
       "      <td>[7]</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1293580602055299072</td>\n",
       "      <td>rt breaking lebanon prosecutor to question sev...</td>\n",
       "      <td>['breaking', 'beirutblast']</td>\n",
       "      <td>[7]</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1292021027543953408</td>\n",
       "      <td>rt according to the lebanese health ministry o...</td>\n",
       "      <td>['beirutblast']</td>\n",
       "      <td>[7]</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1291401752911257606</td>\n",
       "      <td>rt breaking imf urges lebanon to break reform ...</td>\n",
       "      <td>['breaking', 'lebanon']</td>\n",
       "      <td>[7]</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>1291331749956509698</td>\n",
       "      <td>macron says will pitch new political deal to l...</td>\n",
       "      <td>['lebanon']</td>\n",
       "      <td>[7]</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87365</th>\n",
       "      <td>1270406159355334656</td>\n",
       "      <td>rt another fascinating panel this afternoon th...</td>\n",
       "      <td>['trust', 'technology']</td>\n",
       "      <td>[0, 2, 5]</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87379</th>\n",
       "      <td>1268578990220029953</td>\n",
       "      <td>rt join this cogx panel to hear the latest exp...</td>\n",
       "      <td>['cogx2020']</td>\n",
       "      <td>[1, 5]</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87437</th>\n",
       "      <td>1260516465435906050</td>\n",
       "      <td>rt join us today bst to discuss whether a join...</td>\n",
       "      <td>['covid19uk', 'contacttracing']</td>\n",
       "      <td>[2]</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87439</th>\n",
       "      <td>1260119164502659072</td>\n",
       "      <td>rt could ai get us out of lockdown a group of ...</td>\n",
       "      <td>['ai']</td>\n",
       "      <td>[0, 2, 5]</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87447</th>\n",
       "      <td>1258725356988764162</td>\n",
       "      <td>rt view from on contacttracingapp carly kind d...</td>\n",
       "      <td>['contacttracingapp']</td>\n",
       "      <td>[2]</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>722766 rows √ó 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                  tweet_id                                               text  \\\n",
       "1      1293934001514848256  rt breaking via fbi to join beirut blast probe...   \n",
       "2      1293580602055299072  rt breaking lebanon prosecutor to question sev...   \n",
       "6      1292021027543953408  rt according to the lebanese health ministry o...   \n",
       "9      1291401752911257606  rt breaking imf urges lebanon to break reform ...   \n",
       "12     1291331749956509698  macron says will pitch new political deal to l...   \n",
       "...                    ...                                                ...   \n",
       "87365  1270406159355334656  rt another fascinating panel this afternoon th...   \n",
       "87379  1268578990220029953  rt join this cogx panel to hear the latest exp...   \n",
       "87437  1260516465435906050  rt join us today bst to discuss whether a join...   \n",
       "87439  1260119164502659072  rt could ai get us out of lockdown a group of ...   \n",
       "87447  1258725356988764162  rt view from on contacttracingapp carly kind d...   \n",
       "\n",
       "                                     hashtags topic_labels  main_label  \n",
       "1      ['breaking', 'fbi', 'beirut', 'probe']          [7]           7  \n",
       "2                 ['breaking', 'beirutblast']          [7]           7  \n",
       "6                             ['beirutblast']          [7]           7  \n",
       "9                     ['breaking', 'lebanon']          [7]           7  \n",
       "12                                ['lebanon']          [7]           7  \n",
       "...                                       ...          ...         ...  \n",
       "87365                 ['trust', 'technology']    [0, 2, 5]           0  \n",
       "87379                            ['cogx2020']       [1, 5]           1  \n",
       "87437         ['covid19uk', 'contacttracing']          [2]           2  \n",
       "87439                                  ['ai']    [0, 2, 5]           0  \n",
       "87447                   ['contacttracingapp']          [2]           2  \n",
       "\n",
       "[722766 rows x 5 columns]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# this is the cell to run if you want to match topics to keywords\n",
    "\n",
    "labelled_data['topic_labels'] = data_new['keywords'].apply(lambda x: ha.check_for_matches(x, topics, 10))\n",
    "labelled_data['main_label'] = labelled_data['topic_labels'].apply(lambda x: x[0] if len(x)>0 else -1)\n",
    "labelled_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>text</th>\n",
       "      <th>hashtags</th>\n",
       "      <th>topic_labels</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>main_label</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>315379</td>\n",
       "      <td>315379</td>\n",
       "      <td>315379</td>\n",
       "      <td>315379</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>136686</td>\n",
       "      <td>136686</td>\n",
       "      <td>136686</td>\n",
       "      <td>136686</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>85857</td>\n",
       "      <td>85857</td>\n",
       "      <td>85857</td>\n",
       "      <td>85857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>28383</td>\n",
       "      <td>28383</td>\n",
       "      <td>28383</td>\n",
       "      <td>28383</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>25578</td>\n",
       "      <td>25578</td>\n",
       "      <td>25578</td>\n",
       "      <td>25578</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>17576</td>\n",
       "      <td>17576</td>\n",
       "      <td>17576</td>\n",
       "      <td>17576</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>77306</td>\n",
       "      <td>77306</td>\n",
       "      <td>77306</td>\n",
       "      <td>77306</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>36001</td>\n",
       "      <td>36001</td>\n",
       "      <td>36001</td>\n",
       "      <td>36001</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            tweet_id    text  hashtags  topic_labels\n",
       "main_label                                          \n",
       "0             315379  315379    315379        315379\n",
       "1             136686  136686    136686        136686\n",
       "2              85857   85857     85857         85857\n",
       "3              28383   28383     28383         28383\n",
       "4              25578   25578     25578         25578\n",
       "5              17576   17576     17576         17576\n",
       "6              77306   77306     77306         77306\n",
       "7              36001   36001     36001         36001"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# to give an idea about how many tweets are in each topic: \n",
    "labelled_data.groupby('main_label').count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'src.topics.hashtag_analysis' has no attribute 'produce_random_sample'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-33-2ca984241599>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# produce a random sample of topics to train the model with\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0msample_df\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mha\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mproduce_random_sample\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlabelled_data\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabelled_data\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'main_label'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m5000\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m: module 'src.topics.hashtag_analysis' has no attribute 'produce_random_sample'"
     ]
    }
   ],
   "source": [
    "# produce a random sample of topics to train the model with\n",
    "sample_df = ha.produce_random_sample(labelled_data, labelled_data['main_label', 5000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# grouping by label and joining the tweets together to prepare for wordcloud plot\n",
    "grouped = sample_df.groupby('main_label')['text'].agg(' '.join)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plotting a wordcloud of first topic\n",
    "plots.wordcloud_plot(grouped[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## C3: Preprocessing for training Doc2Vec model\n",
    "A lot of code adapted from Martin's script ref: https://towardsdatascience.com/multi-class-text-classification-with-doc2vec-logistic-regression-9da9947b43f4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocessing the tweets for training (no stopword removal since Doc2Vec likes the context for semantic relations)\n",
    "\n",
    "min_characters_sent = 3   #Min characters in a sentence (inclusive)\n",
    "min_characters_word = 3     #Min characters in a word (inclusive)\n",
    "test_size = 0.2     #Fraction of corpus to keep back for testing\n",
    "\n",
    "labelled_data['tokens'] = labelled_data['text'].apply(lambda x: pp.tokenize_text(str(x)), min_characters_word)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = train_test_split(labelled_data[['text', 'main_label', 'tokens']], test_size=test_size, random_state=42)\n",
    "\n",
    "train_tagged = train.apply(\n",
    "    lambda r: TaggedDocument(words=r['tokens'], tags=[str(r.main_label)]), axis=1)\n",
    "test_tagged = test.apply(\n",
    "    lambda r: TaggedDocument(words=r['tokens'], tags=[str(r.main_label)]), axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## C4: Training Doc2Vec model and testing robustness \n",
    "### C4.1: distributed bag of words (dbow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import multiprocessing\n",
    "cores = multiprocessing.cpu_count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model_dbow = Doc2Vec(dm=0, vector_size=300, negative=5, hs=0, min_count=2, sample=0, workers=cores) #Values from tutorial\n",
    "\n",
    "model_dbow = Doc2Vec(dm=0, vector_size=300, negative=5, hs=1, min_count=2, sample=0, workers=cores) #My optimised values\n",
    "\n",
    "model_dbow.build_vocab([x for x in tqdm(train_tagged.values)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "for epoch in range(30):\n",
    "    model_dbow.train(utils.shuffle([x for x in tqdm(train_tagged.values)]), total_examples=len(train_tagged.values), epochs=1)\n",
    "    model_dbow.alpha -= 0.002\n",
    "    model_dbow.min_alpha = model_dbow.alpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train, X_train = tm.vec_for_learning(model_dbow, train_tagged)\n",
    "y_test, X_test = tm.vec_for_learning(model_dbow, test_tagged)\n",
    "\n",
    "logreg_dbow = LogisticRegression(n_jobs=1, C=1e5, max_iter=1000)\n",
    "logreg_dbow.fit(X_train, y_train)\n",
    "y_pred = logreg_dbow.predict(X_test)\n",
    "\n",
    "print('model_dbow Testing accuracy %s' % accuracy_score(y_test, y_pred))\n",
    "print('model_dbow Testing F1 score: {}'.format(f1_score(y_test, y_pred, average='weighted')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### C4.2: distributed memory (dmm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model_dmm = Doc2Vec(dm=1, dm_mean=1, vector_size=300, window=10, negative=5, min_count=1, workers=cores, alpha=0.065, min_alpha=0.065)  #Values from tutorial\n",
    "\n",
    "model_dmm = Doc2Vec(dm=1, dm_mean=0, vector_size=300, window=10, negative=5, min_count=1, workers=cores, alpha=0.1, min_alpha=0) #My optimised values\n",
    "\n",
    "model_dmm.build_vocab([x for x in tqdm(train_tagged.values)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "for epoch in range(30):\n",
    "    model_dmm.train(utils.shuffle([x for x in tqdm(train_tagged.values)]), total_examples=len(train_tagged.values), epochs=1)\n",
    "    model_dmm.alpha -= 0.002\n",
    "    model_dmm.min_alpha = model_dmm.alpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train, X_train = tm.vec_for_learning(model_dmm, train_tagged)\n",
    "y_test, X_test = tm.vec_for_learning(model_dmm, test_tagged)\n",
    "\n",
    "logreg_dmm = LogisticRegression(n_jobs=1, C=1e5, max_iter=1000)\n",
    "logreg_dmm.fit(X_train, y_train)\n",
    "y_pred = logreg_dmm.predict(X_test)\n",
    "\n",
    "print('model_dmm Testing accuracy %s' % accuracy_score(y_test, y_pred))\n",
    "print('model_dmm Testing F1 score: {}'.format(f1_score(y_test, y_pred, average='weighted')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### C4.3: combined pairing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.test.test_doc2vec import ConcatenatedDoc2Vec\n",
    "model_new = ConcatenatedDoc2Vec([model_dbow, model_dmm])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Pairing method\n",
    "\n",
    "y_train, X_train = get_vectors(model_new, train_tagged)\n",
    "y_test, X_test = get_vectors(model_new, test_tagged)\n",
    "\n",
    "logreg_new = LogisticRegression(n_jobs=1, C=1e5, max_iter=1000)\n",
    "logreg_new.fit(X_train, y_train)\n",
    "y_pred = logreg_new.predict(X_test)\n",
    "\n",
    "print('model_new Testing accuracy %s' % accuracy_score(y_test, y_pred))\n",
    "print('model_new Testing F1 score: {}'.format(f1_score(y_test, y_pred, average='weighted')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### C4.4: Testing the model makes sense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sim = model_dmm.n_similarity(['covid'], ['coronavirus'])\n",
    "sim2 = model_dmm.n_similarity(['coronavirus'], ['pandemic'])\n",
    "sim3 = model_dmm.n_similarity(['coronavirus'], ['cybersecurity'])\n",
    "sim4 = model_dmm.n_similarity(['cybersecurity'], ['trump'])\n",
    "\n",
    "print(\"{:.4f}\".format(sim))\n",
    "print(\"{:.4f}\".format(sim2))\n",
    "print(\"{:.4f}\".format(sim3))\n",
    "print(\"{:.4f}\".format(sim4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "similar_words = model_dmm.most_similar(positive=['woman', 'king'], negative=['man'], topn=3)\n",
    "similar_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "similar_words = model_dmm.most_similar(positive=['cryptocurrency', 'phishing'], negative=['security'], topn=3)\n",
    "similar_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "similar_words = model_dmm.most_similar(positive=['media', 'trump'], negative=['safety'], topn=3)\n",
    "similar_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## C5: Apply Doc2Vec model to tweets and some basic analysis\n",
    "### C5.1: apply doc2vec to tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "src_dir = r'C:\\Users\\elizabeth\\Documents\\cleaned tweets\\cyber friends tweets'\n",
    "\n",
    "files = [file for file in os.listdir(src_dir) if os.path.isfile(os.path.join(src_dir, file))]\n",
    "classifier_model = logreg_dbow\n",
    "model = model_dbow\n",
    "\n",
    "with tqdm(total=len(files), desc='Files') as pbar:\n",
    "    i=0\n",
    "    for file in files:\n",
    "        df = pd.read_csv(os.path.join(src_dir, file), low_memory=False)\n",
    "        df = df[df.text != '']\n",
    "        df['clean_tweet'] = df['text'].apply(lambda x: tm.clean_text(x))\n",
    "        test_tagged = df.apply(\n",
    "            lambda r: TaggedDocument(words=tm.tokenize_text(r['clean_tweet']), tags=[r.screen_name]), axis=1)\n",
    "        X_test = tm.get_vectors_apply(model, test_tagged)\n",
    "        df['y_pred'] = classifier_model.predict(X_test)\n",
    "        y_pred_score = classifier_model.predict_proba(X_test)\n",
    "        df2 = pd.DataFrame(y_pred_score)\n",
    "        df2.columns=classifier_model.classes_\n",
    "        df2['score'] = df2.max(axis=1)\n",
    "        df['score'] = df2['score']\n",
    "        df3 = df[['tweet_id','screen_name', 'text', 'y_pred', 'score']].copy()\n",
    "        df4 = pd.concat([df3, df2], axis=1)\n",
    "        df3.to_csv('tweets_trained_topic_modelled_'+str(i)+'.csv', index=False)\n",
    "        df4.to_csv('tweets_trained_all_topics_modelled_'+str(i)+'.csv', index=False)\n",
    "        print(df4.head())\n",
    "        pbar.update(1)\n",
    "        i = i+1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### C5.2: How are user's tweets spread over topics?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "src_dir = r'C:\\Users\\elizabeth\\Documents\\S2DS\\tweets_trained_all_topics_10'\n",
    "\n",
    "files = [file for file in os.listdir(src_dir) if os.path.isfile(os.path.join(src_dir, file))]\n",
    "ls = list(logreg_dbow.classes_)\n",
    "ls.insert(0, 'screen_name')\n",
    "df_totalled_topics = pd.DataFrame(columns=ls)\n",
    "\n",
    "list(df_totalled_topics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tqdm(total=len(files), desc='Files') as pbar:\n",
    "    for file in files:\n",
    "        df = pd.read_csv(os.path.join(src_dir, file), low_memory=False)\n",
    "        df1 = df.groupby('screen_name')['0', '1', '10', '2', '3', '4', '7', '8', '9'].sum()\n",
    "        df_totalled_topics = pd.concat([df_totalled_topics, df1])\n",
    "        pbar.update(1)\n",
    "df_totalled_topics.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_totalled_topics['total'] = df_totalled_topics.sum(axis = 1)\n",
    "df2 = df_totalled_topics[['0', '1', '10', '2', '3', '4', '7', '8', '9']].div(df_totalled_topics.total, axis=0)\n",
    "df2 = df2.reset_index()\n",
    "df2 = df2.rename(columns={'index': 'user_name'})\n",
    "df2.columns = ['user_name', 'Covid19', 'A.I.', 'Other', 'Cybersecurity', 'Financial tech/cyrptocurrency', 'Startup', 'malware', 'environment', 'beirut']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2.to_csv('user_name_topics_summed_10.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3 = df2.set_index('user_name')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = [0, 0.01, 0.02, 0.03, 0.04, 0.05, 0.06]\n",
    "labels = list(df3.columns)\n",
    "row1 = df3.iloc[0]\n",
    "row1.plot(kind='bar',title='Oxchich', color='r',stacked=False, figsize=(15,5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = [0, 0.01, 0.02, 0.03, 0.04, 0.05, 0.06]\n",
    "labels = list(df3.columns)\n",
    "row1 = df3.loc['gcluley']\n",
    "row1.plot(kind='bar',title='gcluley', color='r',stacked=False, figsize=(15,5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### C5.3: Who is the maximum tweeter in each topic?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df3['Covid19'].idxmax(), )\n",
    "print(df3['A.I.'].idxmax(), )\n",
    "print(df3['Cybersecurity'].idxmax(), )\n",
    "print(df3['Financial tech/cyrptocurrency'].idxmax(), )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xlabels = [0.0, 0.1, 0.2, 0.3, 0.4,  0.5, 0.6, 0.7]\n",
    "labels = ['Covid19', 'A.I.', 'Other', 'Cybersecurity', 'Financial tech/cyrptocurrency', 'Startup', 'malware', 'environment', 'beirut']\n",
    "row1 = df3.loc['fuckkgirl_ck']\n",
    "fig = row1.plot(kind='barh', color='r',stacked=False, figsize=(15,5))\n",
    "fig.set_ylabel('Topics', fontsize=30)\n",
    "fig.set_xlabel('Proportion of tweets', fontsize=30)\n",
    "fig.set_yticklabels(labels, fontsize=20)\n",
    "fig.set_xticklabels(xlabels, fontsize=20)\n",
    "fig.set_title('fuckkgirl_ck', fontsize = 30 )\n",
    "fig.figure.savefig('fuckkgirl_ck.png')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
