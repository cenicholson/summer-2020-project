{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: gensim==3.8.3 in c:\\users\\elizabeth\\anaconda3\\envs\\python36\\lib\\site-packages (3.8.3)\n",
      "Requirement already satisfied: smart-open>=1.8.1 in c:\\users\\elizabeth\\anaconda3\\envs\\python36\\lib\\site-packages (from gensim==3.8.3) (2.1.0)\n",
      "Requirement already satisfied: numpy>=1.11.3 in c:\\users\\elizabeth\\anaconda3\\envs\\python36\\lib\\site-packages (from gensim==3.8.3) (1.17.0)\n",
      "Requirement already satisfied: six>=1.5.0 in c:\\users\\elizabeth\\anaconda3\\envs\\python36\\lib\\site-packages (from gensim==3.8.3) (1.15.0)\n",
      "Requirement already satisfied: scipy>=0.18.1 in c:\\users\\elizabeth\\anaconda3\\envs\\python36\\lib\\site-packages (from gensim==3.8.3) (1.4.1)\n",
      "Requirement already satisfied: Cython==0.29.14 in c:\\users\\elizabeth\\anaconda3\\envs\\python36\\lib\\site-packages (from gensim==3.8.3) (0.29.14)\n",
      "Requirement already satisfied: boto3 in c:\\users\\elizabeth\\anaconda3\\envs\\python36\\lib\\site-packages (from smart-open>=1.8.1->gensim==3.8.3) (1.14.47)\n",
      "Requirement already satisfied: requests in c:\\users\\elizabeth\\anaconda3\\envs\\python36\\lib\\site-packages (from smart-open>=1.8.1->gensim==3.8.3) (2.24.0)\n",
      "Requirement already satisfied: boto in c:\\users\\elizabeth\\anaconda3\\envs\\python36\\lib\\site-packages (from smart-open>=1.8.1->gensim==3.8.3) (2.49.0)\n",
      "Requirement already satisfied: botocore<1.18.0,>=1.17.47 in c:\\users\\elizabeth\\anaconda3\\envs\\python36\\lib\\site-packages (from boto3->smart-open>=1.8.1->gensim==3.8.3) (1.17.47)\n",
      "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in c:\\users\\elizabeth\\anaconda3\\envs\\python36\\lib\\site-packages (from boto3->smart-open>=1.8.1->gensim==3.8.3) (0.10.0)\n",
      "Requirement already satisfied: s3transfer<0.4.0,>=0.3.0 in c:\\users\\elizabeth\\anaconda3\\envs\\python36\\lib\\site-packages (from boto3->smart-open>=1.8.1->gensim==3.8.3) (0.3.3)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in c:\\users\\elizabeth\\anaconda3\\envs\\python36\\lib\\site-packages (from requests->smart-open>=1.8.1->gensim==3.8.3) (3.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\elizabeth\\anaconda3\\envs\\python36\\lib\\site-packages (from requests->smart-open>=1.8.1->gensim==3.8.3) (2020.6.20)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in c:\\users\\elizabeth\\anaconda3\\envs\\python36\\lib\\site-packages (from requests->smart-open>=1.8.1->gensim==3.8.3) (1.24.3)\n",
      "Requirement already satisfied: idna<3,>=2.5 in c:\\users\\elizabeth\\anaconda3\\envs\\python36\\lib\\site-packages (from requests->smart-open>=1.8.1->gensim==3.8.3) (2.10)\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in c:\\users\\elizabeth\\anaconda3\\envs\\python36\\lib\\site-packages (from botocore<1.18.0,>=1.17.47->boto3->smart-open>=1.8.1->gensim==3.8.3) (2.8.1)\n",
      "Requirement already satisfied: docutils<0.16,>=0.10 in c:\\users\\elizabeth\\anaconda3\\envs\\python36\\lib\\site-packages (from botocore<1.18.0,>=1.17.47->boto3->smart-open>=1.8.1->gensim==3.8.3) (0.15.2)\n"
     ]
    }
   ],
   "source": [
    "!pip install gensim==3.8.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: wikipedia in c:\\users\\elizabeth\\anaconda3\\envs\\python36\\lib\\site-packages (1.4.0)\n",
      "Requirement already satisfied: beautifulsoup4 in c:\\users\\elizabeth\\anaconda3\\envs\\python36\\lib\\site-packages (from wikipedia) (4.9.1)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.0.0 in c:\\users\\elizabeth\\anaconda3\\envs\\python36\\lib\\site-packages (from wikipedia) (2.24.0)\n",
      "Requirement already satisfied: soupsieve>1.2 in c:\\users\\elizabeth\\anaconda3\\envs\\python36\\lib\\site-packages (from beautifulsoup4->wikipedia) (2.0.1)\n",
      "Requirement already satisfied: idna<3,>=2.5 in c:\\users\\elizabeth\\anaconda3\\envs\\python36\\lib\\site-packages (from requests<3.0.0,>=2.0.0->wikipedia) (2.10)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in c:\\users\\elizabeth\\anaconda3\\envs\\python36\\lib\\site-packages (from requests<3.0.0,>=2.0.0->wikipedia) (3.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\elizabeth\\anaconda3\\envs\\python36\\lib\\site-packages (from requests<3.0.0,>=2.0.0->wikipedia) (2020.6.20)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in c:\\users\\elizabeth\\anaconda3\\envs\\python36\\lib\\site-packages (from requests<3.0.0,>=2.0.0->wikipedia) (1.24.3)\n"
     ]
    }
   ],
   "source": [
    "!pip install wikipedia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import itertools as it\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "tqdm.pandas(desc=\"progress-bar\")\n",
    "from gensim.models import Doc2Vec\n",
    "from sklearn import utils\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import decomposition as skd\n",
    "from sklearn import feature_extraction as skfe\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "import gensim\n",
    "from gensim.models.doc2vec import TaggedDocument\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "from collections import Counter\n",
    "import wikipedia\n",
    "import difflib\n",
    "import nltk\n",
    "#nltk.download('punkt')\n",
    "#nltk.download('stopwords')\n",
    "\n",
    "wikipedia.set_lang('en')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Files: 100%|███████████████████████████████████████████████████████████████████████████| 40/40 [01:48<00:00,  2.70s/it]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>text</th>\n",
       "      <th>hashtags</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1293934001514848256</td>\n",
       "      <td>rt breaking via fbi to join beirut blast probe...</td>\n",
       "      <td>['breaking', 'fbi', 'beirut', 'probe']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1293580602055299072</td>\n",
       "      <td>rt breaking lebanon prosecutor to question sev...</td>\n",
       "      <td>['breaking', 'beirutblast']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1292021027543953408</td>\n",
       "      <td>rt according to the lebanese health ministry o...</td>\n",
       "      <td>['beirutblast']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1291401752911257606</td>\n",
       "      <td>rt breaking imf urges lebanon to break reform ...</td>\n",
       "      <td>['breaking', 'lebanon']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>1291331749956509698</td>\n",
       "      <td>macron says will pitch new political deal to l...</td>\n",
       "      <td>['lebanon']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87365</th>\n",
       "      <td>1270406159355334656</td>\n",
       "      <td>rt another fascinating panel this afternoon th...</td>\n",
       "      <td>['trust', 'technology']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87379</th>\n",
       "      <td>1268578990220029953</td>\n",
       "      <td>rt join this cogx panel to hear the latest exp...</td>\n",
       "      <td>['cogx2020']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87437</th>\n",
       "      <td>1260516465435906050</td>\n",
       "      <td>rt join us today bst to discuss whether a join...</td>\n",
       "      <td>['covid19uk', 'contacttracing']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87439</th>\n",
       "      <td>1260119164502659072</td>\n",
       "      <td>rt could ai get us out of lockdown a group of ...</td>\n",
       "      <td>['ai']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87447</th>\n",
       "      <td>1258725356988764162</td>\n",
       "      <td>rt view from on contacttracingapp carly kind d...</td>\n",
       "      <td>['contacttracingapp']</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>723896 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                  tweet_id                                               text  \\\n",
       "1      1293934001514848256  rt breaking via fbi to join beirut blast probe...   \n",
       "2      1293580602055299072  rt breaking lebanon prosecutor to question sev...   \n",
       "6      1292021027543953408  rt according to the lebanese health ministry o...   \n",
       "9      1291401752911257606  rt breaking imf urges lebanon to break reform ...   \n",
       "12     1291331749956509698  macron says will pitch new political deal to l...   \n",
       "...                    ...                                                ...   \n",
       "87365  1270406159355334656  rt another fascinating panel this afternoon th...   \n",
       "87379  1268578990220029953  rt join this cogx panel to hear the latest exp...   \n",
       "87437  1260516465435906050  rt join us today bst to discuss whether a join...   \n",
       "87439  1260119164502659072  rt could ai get us out of lockdown a group of ...   \n",
       "87447  1258725356988764162  rt view from on contacttracingapp carly kind d...   \n",
       "\n",
       "                                     hashtags  \n",
       "1      ['breaking', 'fbi', 'beirut', 'probe']  \n",
       "2                 ['breaking', 'beirutblast']  \n",
       "6                             ['beirutblast']  \n",
       "9                     ['breaking', 'lebanon']  \n",
       "12                                ['lebanon']  \n",
       "...                                       ...  \n",
       "87365                 ['trust', 'technology']  \n",
       "87379                            ['cogx2020']  \n",
       "87437         ['covid19uk', 'contacttracing']  \n",
       "87439                                  ['ai']  \n",
       "87447                   ['contacttracingapp']  \n",
       "\n",
       "[723896 rows x 3 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#import data\n",
    "\n",
    "src_dir = r'C:\\Users\\elizabeth\\Documents\\cleaned tweets\\cyber friends tweets'\n",
    "\n",
    "files = [file for file in os.listdir(src_dir) if os.path.isfile(os.path.join(src_dir, file))] # build list of files to iterate through\n",
    "\n",
    "data = pd.DataFrame()\n",
    "\n",
    "with tqdm(total=len(files), desc='Files') as pbar:\n",
    "    for file in files:\n",
    "        df = pd.read_csv(os.path.join(src_dir, file), low_memory=False)\n",
    "        data = pd.concat([data, df[['tweet_id', 'text', 'hashtags']]], axis=0)\n",
    "        pbar.update(1)\n",
    "        \n",
    "data = data[data['hashtags'].astype(str) != '[]'] # remove empties\n",
    "data['text'] = data['text'].apply(lambda x: x.lower() if isinstance(x, str) else None)\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Here are some group hashtag topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topics = [['covid19', 'lockdown', 'coronavirus', 'mentalhealth', 'healthcare'], \n",
    "['ai', 'ml', 'dtascience', 'artificalintelligence'], \n",
    "['cybersecurity', 'iot', 'robotics', 'infosec'],\n",
    "['fintech'],\n",
    "['blockchain', 'payments', 'cryptocurrency', 'bitcoin', 'cyrpto'],\n",
    "['innovation', 'tech'],\n",
    "['data', 'gdpr'],\n",
    "['brexit'],\n",
    "['privacy'],\n",
    "['startup company'],\n",
    "['china'],\n",
    "['5g'],\n",
    "['digitaltransformation'],\n",
    "['beirut', 'lebanon'],\n",
    "['belarus'],\n",
    "['journalism', 'travel'], \n",
    "['apple'],\n",
    "['ar', 'vr'], \n",
    "['property', 'realestate'], \n",
    "['architecture', 'design'],\n",
    "['climatechange', 'esg', 'sustainability'],\n",
    "['cybercrime', 'ransomeware', 'malware'],\n",
    "['facebook', 'socialmedia', 'twitter']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### assign topics to tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_for_matches(tag_list, topic_list):\n",
    "    '''\n",
    "    \n",
    "    '''\n",
    "    matches = []\n",
    "    \n",
    "    # Compare list of hashtags in tweet against those in each topic\n",
    "    potential_matches = [bool(set(tag_list).intersection(set(topic))) for topic in topic_list]\n",
    "    if any(potential_matches):\n",
    "        matches.extend([i for i in range(len(potential_matches)) if potential_matches[i]])\n",
    "    else:\n",
    "        matches.append(-1)\n",
    "    return matches\n",
    "\n",
    "possible_topics = data['hashtags'].apply(lambda x: check_for_matches(eval(x), topics))\n",
    "possible_topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labelled_data = data.assign(topic_labels=possible_topics.values)\n",
    "labelled_data['main_label'] = labelled_data['topic_labels'].apply(lambda x: x[0])\n",
    "labelled_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labelled_data['main_label'].value_counts().plot.barh()\n",
    "fig = plt.gcf()\n",
    "fig.set_size_inches(10,15)\n",
    "plt.xscale('log')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "value_counts = labelled_data['main_label'].value_counts()\n",
    "value_counts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "topics = [['covid19', 'lockdown', 'coronavirus', 'mentalhealth', 'healthcare'], \n",
    "['ai', 'ml', 'dtascience', 'artificalintelligence'], \n",
    "['cybersecurity', 'iot', 'robotics', 'infosec'],\n",
    "['fintech', 'blockchain', 'payments', 'cryptocurrency', 'bitcoin', 'cyrpto']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>text</th>\n",
       "      <th>hashtags</th>\n",
       "      <th>topic_labels</th>\n",
       "      <th>main_label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1293934001514848256</td>\n",
       "      <td>rt breaking via fbi to join beirut blast probe...</td>\n",
       "      <td>['breaking', 'fbi', 'beirut', 'probe']</td>\n",
       "      <td>[4]</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1293580602055299072</td>\n",
       "      <td>rt breaking lebanon prosecutor to question sev...</td>\n",
       "      <td>['breaking', 'beirutblast']</td>\n",
       "      <td>[4]</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1292021027543953408</td>\n",
       "      <td>rt according to the lebanese health ministry o...</td>\n",
       "      <td>['beirutblast']</td>\n",
       "      <td>[4]</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1291401752911257606</td>\n",
       "      <td>rt breaking imf urges lebanon to break reform ...</td>\n",
       "      <td>['breaking', 'lebanon']</td>\n",
       "      <td>[4]</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>1291331749956509698</td>\n",
       "      <td>macron says will pitch new political deal to l...</td>\n",
       "      <td>['lebanon']</td>\n",
       "      <td>[4]</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87365</th>\n",
       "      <td>1270406159355334656</td>\n",
       "      <td>rt another fascinating panel this afternoon th...</td>\n",
       "      <td>['trust', 'technology']</td>\n",
       "      <td>[4]</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87379</th>\n",
       "      <td>1268578990220029953</td>\n",
       "      <td>rt join this cogx panel to hear the latest exp...</td>\n",
       "      <td>['cogx2020']</td>\n",
       "      <td>[4]</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87437</th>\n",
       "      <td>1260516465435906050</td>\n",
       "      <td>rt join us today bst to discuss whether a join...</td>\n",
       "      <td>['covid19uk', 'contacttracing']</td>\n",
       "      <td>[4]</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87439</th>\n",
       "      <td>1260119164502659072</td>\n",
       "      <td>rt could ai get us out of lockdown a group of ...</td>\n",
       "      <td>['ai']</td>\n",
       "      <td>[1]</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87447</th>\n",
       "      <td>1258725356988764162</td>\n",
       "      <td>rt view from on contacttracingapp carly kind d...</td>\n",
       "      <td>['contacttracingapp']</td>\n",
       "      <td>[4]</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>723896 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                  tweet_id                                               text  \\\n",
       "1      1293934001514848256  rt breaking via fbi to join beirut blast probe...   \n",
       "2      1293580602055299072  rt breaking lebanon prosecutor to question sev...   \n",
       "6      1292021027543953408  rt according to the lebanese health ministry o...   \n",
       "9      1291401752911257606  rt breaking imf urges lebanon to break reform ...   \n",
       "12     1291331749956509698  macron says will pitch new political deal to l...   \n",
       "...                    ...                                                ...   \n",
       "87365  1270406159355334656  rt another fascinating panel this afternoon th...   \n",
       "87379  1268578990220029953  rt join this cogx panel to hear the latest exp...   \n",
       "87437  1260516465435906050  rt join us today bst to discuss whether a join...   \n",
       "87439  1260119164502659072  rt could ai get us out of lockdown a group of ...   \n",
       "87447  1258725356988764162  rt view from on contacttracingapp carly kind d...   \n",
       "\n",
       "                                     hashtags topic_labels  main_label  \n",
       "1      ['breaking', 'fbi', 'beirut', 'probe']          [4]           4  \n",
       "2                 ['breaking', 'beirutblast']          [4]           4  \n",
       "6                             ['beirutblast']          [4]           4  \n",
       "9                     ['breaking', 'lebanon']          [4]           4  \n",
       "12                                ['lebanon']          [4]           4  \n",
       "...                                       ...          ...         ...  \n",
       "87365                 ['trust', 'technology']          [4]           4  \n",
       "87379                            ['cogx2020']          [4]           4  \n",
       "87437         ['covid19uk', 'contacttracing']          [4]           4  \n",
       "87439                                  ['ai']          [1]           1  \n",
       "87447                   ['contacttracingapp']          [4]           4  \n",
       "\n",
       "[723896 rows x 5 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def check_for_matches(tag_list, topic_list):\n",
    "    '''\n",
    "    \n",
    "    '''\n",
    "    matches = []\n",
    "    \n",
    "    # Compare list of hashtags in tweet against those in each topic\n",
    "    potential_matches = [bool(set(tag_list).intersection(set(topic))) for topic in topics]\n",
    "    if any(potential_matches):\n",
    "        matches.extend([i for i in range(len(potential_matches)) if potential_matches[i]])\n",
    "    else:\n",
    "        matches.append(4)\n",
    "    return matches\n",
    "\n",
    "possible_topics = data['hashtags'].apply(lambda x: check_for_matches(eval(x), topics))\n",
    "\n",
    "labelled_data = data.assign(topic_labels=possible_topics.values)\n",
    "labelled_data['main_label'] = labelled_data['topic_labels'].apply(lambda x: x[0])\n",
    "labelled_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>text</th>\n",
       "      <th>hashtags</th>\n",
       "      <th>topic_labels</th>\n",
       "      <th>main_label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>57949</th>\n",
       "      <td>1290799897470525448</td>\n",
       "      <td>over of the world s workers rely on the inform...</td>\n",
       "      <td>['covid19', 'buildforwardbetter']</td>\n",
       "      <td>[0]</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17074</th>\n",
       "      <td>1238910836514717696</td>\n",
       "      <td>powerful rebuke by of s cowardice insisting th...</td>\n",
       "      <td>['coronavirus']</td>\n",
       "      <td>[0]</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81538</th>\n",
       "      <td>1272540779970469890</td>\n",
       "      <td>hands up who s guilty of junk miles training t...</td>\n",
       "      <td>['training', 'triathlon', 'swimming', 'cycling...</td>\n",
       "      <td>[0]</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65321</th>\n",
       "      <td>1293178121294938113</td>\n",
       "      <td>rt investing in testing contact tracing and pu...</td>\n",
       "      <td>['covid19']</td>\n",
       "      <td>[0]</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5274</th>\n",
       "      <td>1156488530937466880</td>\n",
       "      <td>if you re feeling the pressure this summer you...</td>\n",
       "      <td>['prrequest', 'journorequest', 'burnout', 'men...</td>\n",
       "      <td>[0]</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11063</th>\n",
       "      <td>1187711970708996096</td>\n",
       "      <td>rt at the end of a fascinating st asia europe ...</td>\n",
       "      <td>['asia', 'europe', 'geopolitics', 'connectivity']</td>\n",
       "      <td>[4]</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58547</th>\n",
       "      <td>1286748461258268674</td>\n",
       "      <td>we had so many great submissions for hacktivit...</td>\n",
       "      <td>['hacktivitycon2020']</td>\n",
       "      <td>[4]</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17800</th>\n",
       "      <td>1011627067425873920</td>\n",
       "      <td>h marseille passons la nuit ensemble stonesnof...</td>\n",
       "      <td>['stonesnofilter']</td>\n",
       "      <td>[4]</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16414</th>\n",
       "      <td>1294688845934534656</td>\n",
       "      <td>wonderful wonderful truly wonderful men tomorr...</td>\n",
       "      <td>['tomorrowwillbeagoodday']</td>\n",
       "      <td>[4]</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14078</th>\n",
       "      <td>1286578030513737733</td>\n",
       "      <td>rt today ahe delves into health amp social pol...</td>\n",
       "      <td>['ahe2020']</td>\n",
       "      <td>[4]</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>25000 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                  tweet_id                                               text  \\\n",
       "57949  1290799897470525448  over of the world s workers rely on the inform...   \n",
       "17074  1238910836514717696  powerful rebuke by of s cowardice insisting th...   \n",
       "81538  1272540779970469890  hands up who s guilty of junk miles training t...   \n",
       "65321  1293178121294938113  rt investing in testing contact tracing and pu...   \n",
       "5274   1156488530937466880  if you re feeling the pressure this summer you...   \n",
       "...                    ...                                                ...   \n",
       "11063  1187711970708996096  rt at the end of a fascinating st asia europe ...   \n",
       "58547  1286748461258268674  we had so many great submissions for hacktivit...   \n",
       "17800  1011627067425873920  h marseille passons la nuit ensemble stonesnof...   \n",
       "16414  1294688845934534656  wonderful wonderful truly wonderful men tomorr...   \n",
       "14078  1286578030513737733  rt today ahe delves into health amp social pol...   \n",
       "\n",
       "                                                hashtags topic_labels  \\\n",
       "57949                  ['covid19', 'buildforwardbetter']          [0]   \n",
       "17074                                    ['coronavirus']          [0]   \n",
       "81538  ['training', 'triathlon', 'swimming', 'cycling...          [0]   \n",
       "65321                                        ['covid19']          [0]   \n",
       "5274   ['prrequest', 'journorequest', 'burnout', 'men...          [0]   \n",
       "...                                                  ...          ...   \n",
       "11063  ['asia', 'europe', 'geopolitics', 'connectivity']          [4]   \n",
       "58547                              ['hacktivitycon2020']          [4]   \n",
       "17800                                 ['stonesnofilter']          [4]   \n",
       "16414                         ['tomorrowwillbeagoodday']          [4]   \n",
       "14078                                        ['ahe2020']          [4]   \n",
       "\n",
       "       main_label  \n",
       "57949           0  \n",
       "17074           0  \n",
       "81538           0  \n",
       "65321           0  \n",
       "5274            0  \n",
       "...           ...  \n",
       "11063           4  \n",
       "58547           4  \n",
       "17800           4  \n",
       "16414           4  \n",
       "14078           4  \n",
       "\n",
       "[25000 rows x 5 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_df = pd.DataFrame()\n",
    "for label in range(0,5):\n",
    "    rnd_idx = random.sample(range(labelled_data[labelled_data['main_label']==label].shape[0]), 5000)\n",
    "    sample_df = pd.concat([sample_df, labelled_data[labelled_data['main_label']==label].iloc[rnd_idx]], axis=0)\n",
    "sample_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 2, 3, 4], dtype=int64)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_df['main_label'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "sequence item 111: expected str instance, NoneType found",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-18-81deeff9ae6e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mgrouped\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msample_df\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgroupby\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'main_label'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'text'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0magg\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m' '\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mgrouped\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\python36\\lib\\site-packages\\pandas\\core\\groupby\\generic.py\u001b[0m in \u001b[0;36maggregate\u001b[1;34m(self, func, *args, **kwargs)\u001b[0m\n\u001b[0;32m    263\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    264\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 265\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_python_agg_general\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    266\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mValueError\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    267\u001b[0m                 \u001b[1;31m# TODO: KeyError is raised in _python_agg_general,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\python36\\lib\\site-packages\\pandas\\core\\groupby\\groupby.py\u001b[0m in \u001b[0;36m_python_agg_general\u001b[1;34m(self, func, *args, **kwargs)\u001b[0m\n\u001b[0;32m    934\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    935\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 936\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_python_apply_general\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    937\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    938\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgrouper\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_filter_empty_groups\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\python36\\lib\\site-packages\\pandas\\core\\groupby\\groupby.py\u001b[0m in \u001b[0;36m_python_apply_general\u001b[1;34m(self, f)\u001b[0m\n\u001b[0;32m    750\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    751\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_python_apply_general\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 752\u001b[1;33m         \u001b[0mkeys\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalues\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmutated\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgrouper\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_selected_obj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    753\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    754\u001b[0m         return self._wrap_applied_output(\n",
      "\u001b[1;32m~\\anaconda3\\envs\\python36\\lib\\site-packages\\pandas\\core\\groupby\\ops.py\u001b[0m in \u001b[0;36mapply\u001b[1;34m(self, f, data, axis)\u001b[0m\n\u001b[0;32m    204\u001b[0m             \u001b[1;31m# group might be modified\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    205\u001b[0m             \u001b[0mgroup_axes\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgroup\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maxes\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 206\u001b[1;33m             \u001b[0mres\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgroup\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    207\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0m_is_indexed_like\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mres\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgroup_axes\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    208\u001b[0m                 \u001b[0mmutated\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\python36\\lib\\site-packages\\pandas\\core\\groupby\\groupby.py\u001b[0m in \u001b[0;36m<lambda>\u001b[1;34m(x)\u001b[0m\n\u001b[0;32m    912\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_python_agg_general\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    913\u001b[0m         \u001b[0mfunc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_is_builtin_func\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 914\u001b[1;33m         \u001b[0mf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    915\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    916\u001b[0m         \u001b[1;31m# iterate through \"columns\" ex exclusions to populate output dict\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: sequence item 111: expected str instance, NoneType found"
     ]
    }
   ],
   "source": [
    "grouped = sample_df.groupby('main_label')['text'].agg(' '.join)\n",
    "grouped\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>main_label</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>over of the world s workers rely on the inform...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>microsoft s new app uses ai to help visually i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>rt for the fda digital security category we ar...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>genuinely genuinely fintechrt breaking billion...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                         text\n",
       "main_label                                                   \n",
       "0           over of the world s workers rely on the inform...\n",
       "1           microsoft s new app uses ai to help visually i...\n",
       "2           rt for the fda digital security category we ar...\n",
       "3           genuinely genuinely fintechrt breaking billion...\n",
       "4                                                         NaN"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grouped = sample_df.groupby(['main_label'])[['text']].sum()\n",
    "grouped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install wordcloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os import path\n",
    "from PIL import Image\n",
    "from wordcloud import WordCloud, STOPWORDS, ImageColorGenerator\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wordcloud_plot(text):\n",
    "    wordcloud = WordCloud(collocations=False, \n",
    "                          width = 1000, \n",
    "                          height = 700, \n",
    "                          background_color ='white', \n",
    "                          min_font_size = 10).generate(text)\n",
    "    \n",
    "    plt.figure(figsize = (8,8))\n",
    "    plt.imshow(wordcloud, interpolation='bilinear')\n",
    "    plt.axis(\"off\")\n",
    "    plt.tight_layout(pad = 0)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordcloud_plot(grouped['text'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordcloud_plot(grouped['text'][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordcloud_plot(grouped['text'][2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordcloud_plot(grouped['text'][3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordcloud_plot(grouped['text'][4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "def tokenize_text(text):\n",
    "    tokens = []\n",
    "    for sent in nltk.sent_tokenize(text):\n",
    "        for word in nltk.word_tokenize(sent):\n",
    "            if (len(word) >= min_characters_word) & (word not in stop_words) : tokens.append(word)\n",
    "#            if (len(word) >= min_characters_word) : tokens.append(word)\n",
    "    return tokens\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre-process training data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adapted from Martin's code ref:  https://towardsdatascience.com/multi-class-text-classification-with-doc2vec-logistic-regression-9da9947b43f4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_characters_sent = 3   #Min characters in a paragraph (inclusive)\n",
    "min_characters_word = 3     #Min characters in a word (inclusive)\n",
    "test_size = 0.2     #Fraction of corpus to keep back for testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time labelled_data['tokens'] = labelled_data['text'].apply(lambda x: tokenize_text(str(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = train_test_split(labelled_data[['text', 'main_label', 'tokens']], test_size=test_size, random_state=42)\n",
    "\n",
    "train_tagged = train.apply(\n",
    "    lambda r: TaggedDocument(words=r['tokens'], tags=[str(r.main_label)]), axis=1)\n",
    "test_tagged = test.apply(\n",
    "    lambda r: TaggedDocument(words=r['tokens'], tags=[str(r.main_label)]), axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Apply models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# # Distributed bag of words\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import multiprocessing\n",
    "cores = multiprocessing.cpu_count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model_dbow = Doc2Vec(dm=0, vector_size=300, negative=5, hs=0, min_count=2, sample=0, workers=cores) #Values from tutorial\n",
    "\n",
    "model_dbow = Doc2Vec(dm=0, vector_size=500, negative=5, hs=1, min_count=2, sample=0, workers=cores) #My optimised values\n",
    "\n",
    "model_dbow.build_vocab([x for x in tqdm(train_tagged.values)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "for epoch in range(30):\n",
    "    model_dbow.train(utils.shuffle([x for x in tqdm(train_tagged.values)]), total_examples=len(train_tagged.values), epochs=1)\n",
    "    model_dbow.alpha -= 0.002\n",
    "    model_dbow.min_alpha = model_dbow.alpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vec_for_learning(model, tagged_docs):\n",
    "    sents = tagged_docs.values\n",
    "    targets, regressors = zip(*[(doc.tags[0], model.infer_vector(doc.words, steps=20)) for doc in sents])\n",
    "    return targets, regressors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distributed Bag of Words (DBOW) model\n",
    "\n",
    "y_train, X_train = vec_for_learning(model_dbow, train_tagged)\n",
    "y_test, X_test = vec_for_learning(model_dbow, test_tagged)\n",
    "logreg_dbow = LogisticRegression(n_jobs=1, C=1e5, max_iter=1000)\n",
    "logreg_dbow.fit(X_train, y_train)\n",
    "y_pred = logreg_dbow.predict(X_test)\n",
    "print('xgb_model_dbow Testing accuracy %s' % accuracy_score(y_test, y_pred))\n",
    "print('xgb_model_dbow Testing F1 score: {}'.format(f1_score(y_test, y_pred, average='weighted')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Distributed memory\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model_dmm = Doc2Vec(dm=1, dm_mean=1, vector_size=300, window=10, negative=5, min_count=1, workers=cores, alpha=0.065, min_alpha=0.065)  #Values from tutorial\n",
    "\n",
    "model_dmm = Doc2Vec(dm=1, dm_mean=0, vector_size=500, window=10, negative=5, min_count=1, workers=cores, alpha=0.1, min_alpha=0) #My optimised values\n",
    "\n",
    "model_dmm.build_vocab([x for x in tqdm(train_tagged.values)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "for epoch in range(30):\n",
    "    model_dmm.train(utils.shuffle([x for x in tqdm(train_tagged.values)]), total_examples=len(train_tagged.values), epochs=1)\n",
    "    model_dmm.alpha -= 0.002\n",
    "    model_dmm.min_alpha = model_dmm.alpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distributed Memory (DM) model\n",
    "\n",
    "y_train, X_train = vec_for_learning(model_dmm, train_tagged)\n",
    "y_test, X_test = vec_for_learning(model_dmm, test_tagged)\n",
    "\n",
    "logreg_dmm = LogisticRegression(n_jobs=1, C=1e5, max_iter=1000)\n",
    "logreg_dmm.fit(X_train, y_train)\n",
    "y_pred = logreg_dmm.predict(X_test)\n",
    "\n",
    "print('model_dmm Testing accuracy %s' % accuracy_score(y_test, y_pred))\n",
    "print('model_dmm Testing F1 score: {}'.format(f1_score(y_test, y_pred, average='weighted')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Combined model pairing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.test.test_doc2vec import ConcatenatedDoc2Vec\n",
    "model_new = ConcatenatedDoc2Vec([model_dbow, model_dmm])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_vectors(model, tagged_docs):\n",
    "    sents = tagged_docs.values\n",
    "    targets, regressors = zip(*[(doc.tags[0], model.infer_vector(doc.words, steps=20)) for doc in sents])\n",
    "    return targets, regressors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Pairing method\n",
    "\n",
    "y_train, X_train = get_vectors(model_new, train_tagged)\n",
    "y_test, X_test = get_vectors(model_new, test_tagged)\n",
    "logreg_new = LogisticRegression(n_jobs=1, C=1e5, max_iter=1000)\n",
    "logreg_new.fit(X_train, y_train)\n",
    "y_pred = logreg_new.predict(X_test)\n",
    "print('xgb_model_new Testing accuracy %s' % accuracy_score(y_test, y_pred))\n",
    "print('xgb_model_new Testing F1 score: {}'.format(f1_score(y_test, y_pred, average='weighted')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Free up memory (not necessary for our small training sample)\n",
    "model_dbow.delete_temporary_training_data(keep_doctags_vectors=True, keep_inference=True)\n",
    "model_dmm.delete_temporary_training_data(keep_doctags_vectors=True, keep_inference=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Apply to tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_vectors_apply(model, docs_to_classify):\n",
    "    sents = docs_to_classify.values\n",
    "    regressors = [model.infer_vector(doc.words, steps=20) for doc in sents]\n",
    "    return regressors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    text = re.sub(r\"http\\S+\", \"\", str(text)) #remove urls\n",
    "    text = re.sub(r'\\S+\\.com\\S+','',str(text)) #remove urls\n",
    "    text = re.sub(r'\\@\\w+','',str(text)) #remove mentions\n",
    "    text =re.sub(r'\\#','',str(text)) #remove hashtags\n",
    "    text = re.findall(r'[A-Za-z]+',str(text))\n",
    "    text = ' '.join(text) \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "src_dir = r'C:\\Users\\elizabeth\\Documents\\cleaned tweets\\cyber friends tweets'\n",
    "\n",
    "files = [file for file in os.listdir(src_dir) if os.path.isfile(os.path.join(src_dir, file))]\n",
    "classifier_model = logreg_dbow\n",
    "model = model_new\n",
    "\n",
    "with tqdm(total=len(files), desc='Files') as pbar:\n",
    "    i=0\n",
    "    for file in files[:2]:\n",
    "        df = pd.read_csv(os.path.join(src_dir, file), low_memory=False)\n",
    "        df = df[df.text != '']\n",
    "        df['clean_tweet'] = df['text'].apply(lambda x: clean_text(x))\n",
    "        test_tagged = df.apply(\n",
    "            lambda r: TaggedDocument(words=tokenize_text(r['clean_tweet']), tags=[r.screen_name]), axis=1)\n",
    "        X_test = get_vectors_apply(model, test_tagged)\n",
    "        df['y_pred'] = classifier_model.predict(X_test)\n",
    "        y_pred_score = classifier_model.predict_proba(X_test)\n",
    "        df2 = pd.DataFrame(y_pred_score)\n",
    "        df2.columns=classifier_model.classes_\n",
    "        df2['score'] = df2.max(axis=1)\n",
    "        df['score'] = df2['score']\n",
    "        df3 = df[['tweet_id','screen_name', 'text', 'y_pred', 'score']].copy()\n",
    "        df4 = pd.concat([df3, df2], axis=1)\n",
    "        df3.to_csv('tweets_trained_topic_modelled_'+str(i)+'.csv', index=False)\n",
    "        df4.to_csv('tweets_trained_all_topics_modelled_'+str(i)+'.csv', index=False)\n",
    "        print(df4.head())\n",
    "        pbar.update(1)\n",
    "        i = i+1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How are journalists interests are spread?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "first we need to drop the row where the classification has been less than 0.5, then we can pool over user_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "src_dir = r'C:\\Users\\elizabeth\\Documents\\S2DS\\tweets_trained_all_topics'\n",
    "\n",
    "files = [file for file in os.listdir(src_dir) if os.path.isfile(os.path.join(src_dir, file))]\n",
    "ls = list(logreg_dbow.classes_)\n",
    "ls.insert(0, 'screen_name')\n",
    "df_totalled_topics = pd.DataFrame(columns=ls)\n",
    "\n",
    "list(df_totalled_topics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topics = [['covid19', 'lockdown', 'coronavirus', 'mentalhealth', 'healthcare'], \n",
    "['ai', 'ml', 'dtascience', 'artificalintelligence'], \n",
    "['cybersecurity', 'iot', 'robotics', 'infosec'],\n",
    "['fintech'],\n",
    "['blockchain', 'payments', 'cryptocurrency', 'bitcoin', 'cyrpto']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tqdm(total=len(files), desc='Files') as pbar:\n",
    "    for file in files:\n",
    "        df = pd.read_csv(os.path.join(src_dir, file), low_memory=False)\n",
    "        df1 = df.groupby('screen_name')['-1', '0', '1', '10',\n",
    "                                        '11', '12', '13', '14', '15',\n",
    "                                        '16', '17', '18', '19', '2',\n",
    "                                        '20', '21', '22', '3', '4',\n",
    "                                        '5', '6', '7', '8'].sum()\n",
    "        df_totalled_topics = pd.concat([df_totalled_topics, df1])\n",
    "        pbar.update(1)\n",
    "df_totalled_topics.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_totalled_topics['total'] = df_totalled_topics.sum(axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = df_totalled_topics[['-1', '0', '1', '10',\n",
    "                                        '11', '12', '13', '14', '15',\n",
    "                                        '16', '17', '18', '19', '2',\n",
    "                                        '20', '21', '22', '3', '4',\n",
    "                                        '5', '6', '7', '8']].div(df_totalled_topics.total, axis=0)\n",
    "df2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = df2.reset_index()\n",
    "df2 = df2.rename(columns={'index': 'user_name'})\n",
    "df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2.to_csv('user_name_topics_summed.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3 = df2.set_index('user_name')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "x = [0, 0.01, 0.02, 0.03, 0.04, 0.05, 0.06]\n",
    "labels = list(df3.columns)\n",
    "row1 = df3.iloc[0]\n",
    "row1.plot(kind='bar',title='Oxchich', color='r',stacked=False, figsize=(15,5))\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = [0, 0.01, 0.02, 0.03, 0.04, 0.05, 0.06]\n",
    "labels = list(df3.columns)\n",
    "row1 = df3.iloc[1]\n",
    "row1.plot(kind='bar',title='_benkatz', color='r',stacked=False, figsize=(15,5))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = [0, 0.01, 0.02, 0.03, 0.04, 0.05, 0.06]\n",
    "labels = list(df3.columns)\n",
    "row1 = df3.iloc[2]\n",
    "row1.plot(kind='bar',title='_john_handel', color='r',stacked=False, figsize=(15,5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = [0, 0.01, 0.02, 0.03, 0.04, 0.05, 0.06]\n",
    "labels = list(df3.columns)\n",
    "row1 = df3.loc['gcluley']\n",
    "row1.plot(kind='bar',title='_john_handel', color='r',stacked=False, figsize=(15,5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Wordclouds for a sense check "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install wordcloud\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os import path\n",
    "from PIL import Image\n",
    "from wordcloud import WordCloud, STOPWORDS, ImageColorGenerator\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wordcloud_plot(text):\n",
    "    wordcloud = WordCloud(collocations=False, \n",
    "                          width = 1000, \n",
    "                          height = 700, \n",
    "                          background_color ='white', \n",
    "                          min_font_size = 10).generate(text)\n",
    "    \n",
    "    plt.figure(figsize = (8,8))\n",
    "    plt.imshow(wordcloud, interpolation='bilinear')\n",
    "    plt.axis(\"off\")\n",
    "    plt.tight_layout(pad = 0)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "src_dir = r'C:\\Users\\elizabeth\\Documents\\S2DS\\tweets_trained_topic_modelled'\n",
    "\n",
    "files = [file for file in os.listdir(src_dir) if os.path.isfile(os.path.join(src_dir, file))]\n",
    "with tqdm(total=len(files), desc='Files') as pbar:\n",
    "    for file in files[:5]:\n",
    "        df = pd.read_csv(os.path.join(src_dir, file), low_memory=False)\n",
    "        df1 = df.groupby(['y_pred'])[['text']].sum()\n",
    "        pbar.update(1)\n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1.head(3).style.set_properties(subset=['text'], **{'width':'1000px'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordcloud_plot(df1['text'][0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordcloud_plot(df1['text'][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordcloud_plot(df1['text'][2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordcloud_plot(df1['text'][3])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
