{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Ditchley S2DS project August 2020 - Code Pipeline<h1>\n",
    "    <h2>Team: Adam Hawken, Luca Lamoni, Elizabeth Nicholson, Robert Webster<h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#![]() #graphical representation of the pipeline here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data directory & sub-directories already exist, skipping.\n"
     ]
    }
   ],
   "source": [
    "# Import modules and set up working directory\n",
    "import sys\n",
    "import os\n",
    "import time\n",
    "import logging\n",
    "import json\n",
    "import csv\n",
    "import threading\n",
    "import queue\n",
    "import asyncio \n",
    "import nest_asyncio\n",
    "nest_asyncio.apply()\n",
    "import twint\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "# Set up working directory\n",
    "# The working directory should reflect the structure of the Github repository https://github.com/S2DSLondon/Aug20_Ditchley\n",
    "sys.path.insert(1, 'C:/Users/Luca/Aug20_Ditchley/')\n",
    "from src.data import pipeline_setup\n",
    "pipeline_setup.build_data_dir('C:/Users/Luca/Aug20_Ditchley/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Section 6.1: Import modules and initialize graph database<h4> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import standard libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from py2neo import Graph\n",
    "from py2neo.data import Node, Relationship\n",
    "from src.data import graphdb as gdb\n",
    "\n",
    "# load / declare the database\n",
    "graph = gdb.get_graph(new_graph = True)\n",
    "graph\n",
    "# start with an empty graph\n",
    "graph.delete_all()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Section 1: Getting journalist twitter handles according to a keyword<h3>\n",
    "    <h4>The journalist scraping is performed at the web address https://www.journalism.co.uk/prof/?chunk=0&cmd=default<h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Choose keyword and run the scraping function\n",
    "from src.data import journalists as journos\n",
    "keyword = 'cybersecurity'\n",
    "# Input: string / Output: list\n",
    "journo_handles = journos.get_handles_by_keyword(keyword)\n",
    "print(len(journo_handles))\n",
    "type(journo_handles)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Section 2. Scrape user information and friend lists for each journalist in the list<h3>\n",
    "    <h4>Section 2.1: Scrape user information using the Twitter API<h4> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load twitter API credentials and return a tweepy API instance\n",
    "import json\n",
    "import tweepy\n",
    "from src.data import api_tweepy as api\n",
    "\n",
    "# Input: path of json file with credentials / Output: tweepy.api.API\n",
    "tw_api = api.connect_API('../src/data/twitter_credentials.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>screen_name</th>\n",
       "      <th>name</th>\n",
       "      <th>location</th>\n",
       "      <th>user_description</th>\n",
       "      <th>user_friends_n</th>\n",
       "      <th>user_followers_n</th>\n",
       "      <th>prof_created_at</th>\n",
       "      <th>favourites_count</th>\n",
       "      <th>verified</th>\n",
       "      <th>statuses_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>335773502</td>\n",
       "      <td>_lucyingham</td>\n",
       "      <td>Lucy Ingham</td>\n",
       "      <td>London</td>\n",
       "      <td>Editor of and digital magazines Verdict Magazi...</td>\n",
       "      <td>513</td>\n",
       "      <td>646</td>\n",
       "      <td>2011-07-15 06:29:08</td>\n",
       "      <td>2150</td>\n",
       "      <td>False</td>\n",
       "      <td>452</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>964233746865119233</td>\n",
       "      <td>jesscahaworth</td>\n",
       "      <td>Jessica Haworth</td>\n",
       "      <td></td>\n",
       "      <td>Cybersecurity journalist at Music buff and ski...</td>\n",
       "      <td>970</td>\n",
       "      <td>670</td>\n",
       "      <td>2018-02-15 20:23:34</td>\n",
       "      <td>453</td>\n",
       "      <td>False</td>\n",
       "      <td>576</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1186245031507693574</td>\n",
       "      <td>ad_nauseum74</td>\n",
       "      <td>Adam Bannister</td>\n",
       "      <td></td>\n",
       "      <td>Journalist The Daily Swig Cybersecurity</td>\n",
       "      <td>366</td>\n",
       "      <td>133</td>\n",
       "      <td>2019-10-21 11:38:12</td>\n",
       "      <td>112</td>\n",
       "      <td>False</td>\n",
       "      <td>277</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               user_id    screen_name             name location  \\\n",
       "0            335773502    _lucyingham      Lucy Ingham   London   \n",
       "1   964233746865119233  jesscahaworth  Jessica Haworth            \n",
       "2  1186245031507693574   ad_nauseum74   Adam Bannister            \n",
       "\n",
       "                                    user_description  user_friends_n  \\\n",
       "0  Editor of and digital magazines Verdict Magazi...             513   \n",
       "1  Cybersecurity journalist at Music buff and ski...             970   \n",
       "2            Journalist The Daily Swig Cybersecurity             366   \n",
       "\n",
       "   user_followers_n     prof_created_at  favourites_count  verified  \\\n",
       "0               646 2011-07-15 06:29:08              2150     False   \n",
       "1               670 2018-02-15 20:23:34               453     False   \n",
       "2               133 2019-10-21 11:38:12               112     False   \n",
       "\n",
       "   statuses_count  \n",
       "0             452  \n",
       "1             576  \n",
       "2             277  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Scrape user information using the API\n",
    "from src.data import api_user_tools as api_tools\n",
    "from src.data import data_cleanup as dc\n",
    "\n",
    "# Input: tweepy.api.API,list / Output: list\n",
    "api_users = api_tools.batch_request_user_info(tw_api,journo_handles)\n",
    "# Input: list / Output: DataFrame\n",
    "df_api = dc.populate_user_df(api_users)\n",
    "# Check\n",
    "df_api.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the dataframe as csv\n",
    "df_api.to_csv('../data/processed/'+keyword+'_user_profiles.csv', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Section 6.2: Load user info into graph DB<h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading in user information and drawing (Person) nodes\n"
     ]
    }
   ],
   "source": [
    "# Neo4j import files need to be in a specific folder, however, the csv files saved above are in a different folder, to go around this problem on Windows machines it is\n",
    "# possible to create a shortcut between the two folders\n",
    "\n",
    "# lowd in user information\n",
    "print('Loading in user information and drawing (Person) nodes')\n",
    "fn_users = 'cybersecurity_user_profiles.csv'\n",
    "gdb.load_users(fn_users ,graph)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Section 2.2: Scrape user friend list using Twint<h4> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \n",
    "from src.data import twint_tools as tt\n",
    "\n",
    "# define keyword arguments / 'n_retries' = max number of scrape attempts, 'suppress' = hide critical Twint warnings\n",
    "kwargs = {'n_retries':5,\n",
    "         'suppress':False}\n",
    "# Multi threading function Input: _get_friends function, number of threads to distribute the queque, args and kwargs\n",
    "tt.twint_in_queue(tt._get_friends, 6, journo_handles, args=('../data/raw/'+keyword+'_',), kwargs=kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenate all the individual lists into one dataframe with journalist and its friends\n",
    "friends_csv = tt.join_friends_csv(journo_handles,keyword) # this function has a bug, the first friend name is 'username'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the dataframe\n",
    "friends_csv.to_csv('../data/processed/'+keyword+'_journalist_friends.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Section 3. Loop over selected journalists handles and scrape their tweets (3.1) and mentions (3.2) using Twint<h3>\n",
    "    <h4>Section 3.1: Scrape tweets using Twint<h4> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.data import twint_tools as tt\n",
    "# define keyword arguments\n",
    "kwargs = {'date_range':('2020-08-01 00:00:00', None),\n",
    "         'n_retries':5,\n",
    "         'suppress':False}\n",
    "# multi threading\n",
    "tt.twint_in_queue(tt._search_tweets_by_user, 3, journo_handles, args=('../data/raw/'+keyword+'_',), kwargs=kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Joined all the individual csv into one dataframe\n",
    "cyber_test = tt.join_tweet_csv(journo_handles, keyword)\n",
    "# Check\n",
    "cyber_test.head()\n",
    "\n",
    "\n",
    "##########################save!!!!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Section 3.2: Extract mentions from Twint dataset<h4> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from the twint dataset, extract mentions based on tweet id and save in a separate csv\n",
    "mentions_twint  = dc.twint_mentions_to_df(cyber_test)\n",
    "# Check\n",
    "mentions_twint.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the dataframe\n",
    "mentions_twint.to_csv('../data/processed/' + keyword + '_mentions_twint.csv',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Section 4. Loop over selected journalists handles and scrape their tweets (4.1) and mentions (4.2) using Twitter API<h3>\n",
    "    <h4>Section 4.1: Scrape tweets using Twint ################ I am waiting for Rob function here<h4> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import tweepy\n",
    "from src.data import api_tweepy as api\n",
    "#Load twitter API credentials and return a tweepy API instance\n",
    "tw_api = api.connect_API('../src/data/twitter_credentials.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Section 4.2: Extract mentions from API tweets<h4> \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Section 5. Data cleaning and standardization/LDA<h3>\n",
    "     <h4>Section 5.1: Clean and standardize Twint dataset<h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardise the twint output \n",
    "from src.data import data_cleanup as dc\n",
    "#Create the standardized template\n",
    "test_twint = dc.standard_tweet_dataset_setup()\n",
    "test_twint\n",
    "#fill the template\n",
    "standard_tweet_twint = dc.fill_standard_tweet_dataset_with_twint(test_twint, cyber_test)\n",
    "# Check\n",
    "standard_tweet_twint.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the dataframe\n",
    "standard_tweet_twint.to_csv('../data/processed/' + keyword + '_standard_tweets_twint.csv',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Section 5.2: Clean and standardize API dataset<h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add here new cleaning function Rob is working on"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Section 6. Create graph database and import twitter data into it<h3>\n",
    "    <h4>Section 6.1: Import modules and load graph database<h4> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import standard libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from py2neo import Graph\n",
    "from py2neo.data import Node, Relationship\n",
    "from src.data import graphdb as gdb\n",
    "\n",
    "# load / declare the database\n",
    "graph = gdb.get_graph(new_graph = True)\n",
    "graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Section 6.2: Load user info into graph DB<h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Neo4j import files need to be in a specific folder, however, the csv files saved above are in a different folder, to go around this problem on Windows machines it is\n",
    "# possible to create a shortcut between the two folders\n",
    "\n",
    "# lowd in user information\n",
    "print('Loading in user information and drawing (Person) nodes')\n",
    "fn_users = 'cybersecurity_user_profiles.csv'\n",
    "gdb.load_users(fn_users ,graph)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Section 6.2: Load friend information into DB<h4> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load in friend information\n",
    "print('Loading in friends info and drawing [FOLLOWS] edges')\n",
    "fn_friends = 'cybersecurity_journalist_friends.csv'\n",
    "gdb.load_friends(fn_friends,graph)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Section 6.3: Load tweet data into DB<h4> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load in tweet information\n",
    "print('Loading in tweets and drawing (Tweet) nodes')\n",
    "fn_tweets = '/data/processed/cybersecurity_standard_tweets_twint.csv'\n",
    "gdb.load_tweets(fn_tweets ,graph) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Section 6.4: Draw edges between users and their tweets<h4> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# draw edges between users and their tweets\n",
    "print('Drawing [POSTS] edges')\n",
    "gdb.get_posts(graph)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Section 6.5: Load tweets' mentions<h4> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load in mentions information\n",
    "print('Loading in mentions and drawing [MENTIONS] edges')\n",
    "fn_mentions = 'cybersecurity_mentions_twint.csv'\n",
    "gdb.load_mentions(fn_mentions,graph)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Section 6.6: Run page rank algorithm using [FOLLOWS] [MENTIONS] edges<h4> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run Page rank using follower and mention edges\n",
    "print('running page rank')\n",
    "nodelist = ['Person','Tweet']\n",
    "edgelist = ['FOLLOWS','MENTIONS']\n",
    "page_rank_friends_mentions = gdb.run_pagerank(nodelist,edgelist,graph)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Section 6.7: Get a weighted random sample from the journalists friends<h4> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get a weighted random sample of users\n",
    "n_sample = 20\n",
    "fields = ['rank']\n",
    "exponents = [2]\n",
    "sample = gdb.get_multiple_weighted_sample(page_rank_friends_mentions,n_sample,fields,exponents)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
