{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Ditchley S2DS project August 2020 - Code Pipeline<h1>\n",
    "    <h2>Team: Adam Hawken, Luca Lamoni, Elizabeth Nicholson, Robert Webster<h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![]() #graphical representation of the pipeline here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import modules and set up working directory\n",
    "import sys\n",
    "import os\n",
    "import time\n",
    "import logging\n",
    "import json\n",
    "import csv\n",
    "import threading\n",
    "import queue\n",
    "import asyncio \n",
    "import nest_asyncio\n",
    "nest_asyncio.apply()\n",
    "import twint\n",
    "import pandas as pd\n",
    "\n",
    "sys.path.insert(1, 'C:/Users/Luca/Aug20_Ditchley')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Section 1: Getting journalist twitter handles according to a keyword<h3>\n",
    "    <h4>The journalist scraping is performed at the web address https://www.journalism.co.uk/prof/?chunk=0&cmd=default<h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n"
     ]
    }
   ],
   "source": [
    "from src.data import journalists as journos\n",
    "keyword = 'cybersecurity'\n",
    "journo_handles = journos.get_handles_by_keyword(keyword)\n",
    "print(len(journo_handles))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Section 2. Scrape user information and friend lists for each journalist in the list<h3>\n",
    "    <h4>Section 2.1: Scrape user information using the Twitter API<h4> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import tweepy\n",
    "from src.data import api_tweepy as api\n",
    "#Load twitter API credentials and return a tweepy API instance\n",
    "tw_api = api.connect_API('../src/data/twitter_credentials.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>screen_name</th>\n",
       "      <th>name</th>\n",
       "      <th>location</th>\n",
       "      <th>user_description</th>\n",
       "      <th>user_friends_n</th>\n",
       "      <th>user_followers_n</th>\n",
       "      <th>prof_created_at</th>\n",
       "      <th>favourites_count</th>\n",
       "      <th>verified</th>\n",
       "      <th>statuses_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>335773502</td>\n",
       "      <td>_lucyingham</td>\n",
       "      <td>Lucy Ingham</td>\n",
       "      <td>London</td>\n",
       "      <td>Editor of @verdictuk and digital magazines #Ve...</td>\n",
       "      <td>512</td>\n",
       "      <td>645</td>\n",
       "      <td>2011-07-15 06:29:08</td>\n",
       "      <td>2118</td>\n",
       "      <td>False</td>\n",
       "      <td>448</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>964233746865119233</td>\n",
       "      <td>jesscahaworth</td>\n",
       "      <td>Jessica Haworth</td>\n",
       "      <td></td>\n",
       "      <td>Cybersecurity journalist at @DailySwig. Music ...</td>\n",
       "      <td>969</td>\n",
       "      <td>669</td>\n",
       "      <td>2018-02-15 20:23:34</td>\n",
       "      <td>452</td>\n",
       "      <td>False</td>\n",
       "      <td>572</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1186245031507693574</td>\n",
       "      <td>ad_nauseum74</td>\n",
       "      <td>Adam Bannister</td>\n",
       "      <td></td>\n",
       "      <td>Journalist, The Daily Swig. Cybersecurity.</td>\n",
       "      <td>366</td>\n",
       "      <td>132</td>\n",
       "      <td>2019-10-21 11:38:12</td>\n",
       "      <td>112</td>\n",
       "      <td>False</td>\n",
       "      <td>276</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               user_id    screen_name             name location  \\\n",
       "0            335773502    _lucyingham      Lucy Ingham   London   \n",
       "1   964233746865119233  jesscahaworth  Jessica Haworth            \n",
       "2  1186245031507693574   ad_nauseum74   Adam Bannister            \n",
       "\n",
       "                                    user_description  user_friends_n  \\\n",
       "0  Editor of @verdictuk and digital magazines #Ve...             512   \n",
       "1  Cybersecurity journalist at @DailySwig. Music ...             969   \n",
       "2         Journalist, The Daily Swig. Cybersecurity.             366   \n",
       "\n",
       "   user_followers_n     prof_created_at  favourites_count  verified  \\\n",
       "0               645 2011-07-15 06:29:08              2118     False   \n",
       "1               669 2018-02-15 20:23:34               452     False   \n",
       "2               132 2019-10-21 11:38:12               112     False   \n",
       "\n",
       "   statuses_count  \n",
       "0             448  \n",
       "1             572  \n",
       "2             276  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Scrape user information using the API\n",
    "from src.data import api_user_tools as api_tools\n",
    "from src.data import data_cleanup as dc\n",
    "api_users = api_tools.batch_request_user_info(tw_api,journo_handles)\n",
    "df_api = dc.populate_user_df(api_users)\n",
    "# Check\n",
    "df_api.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the dataframe\n",
    "df_api.to_csv('../data/raw/'+keyword+'_user_profiles.csv', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Section 2.2: Scrape user friend list using Twint<h4> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.data import twint_tools as tt\n",
    "# define keyword arguments\n",
    "kwargs = {'n_retries':5,\n",
    "         'suppress':False}\n",
    "# multi threading\n",
    "tt.twint_in_queue(tt._get_friends, 3, journo_handles, args=('../data/raw/'+keyword+'_',), kwargs=kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "@_lucyingham follows 1024 users.\n",
      "@JesscaHaworth follows 1939 users.\n",
      "@Ad_Nauseum74 follows 733 users.\n",
      "\n",
      "Total number of handles pulled: 3696\n",
      "Number of unique twitter handles: 1709\n",
      "\n",
      "Zero following in list for users: []\n"
     ]
    }
   ],
   "source": [
    "# Concatenate all the individual lists into one dataframe with journalist and its friends\n",
    "friends_csv = tt.join_friends_csv(journo_handles,keyword) # this function has a bug, the first friend name is 'username'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the dataframe\n",
    "friends_csv.to_csv('../data/raw/'+keyword+'_journalist_friends.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Section 3. Loop over selected journalists handles and scrape their tweets (3.1) and mentions (3.2) using Twint<h3>\n",
    "    <h4>Section 3.1: Scrape tweets using Twint<h4> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attempt #1 to get tweets of @_lucyingham\n",
      "Attempt #1 to get tweets of @JesscaHaworth\n",
      "Attempt #1 to get tweets of @Ad_Nauseum74\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "CRITICAL:root:twint.get:User:'NoneType' object is not subscriptable\n",
      "CRITICAL:root:twint.get:User:'NoneType' object is not subscriptable\n",
      "CRITICAL:root:twint.get:User:'NoneType' object is not subscriptable\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results for @Ad_Nauseum74 saved to: ../data/raw/cybersecurity_tweets_Ad_Nauseum74.csv\n",
      "Results for @JesscaHaworth saved to: ../data/raw/cybersecurity_tweets_JesscaHaworth.csv\n",
      "Results for @_lucyingham saved to: ../data/raw/cybersecurity_tweets__lucyingham.csv\n"
     ]
    }
   ],
   "source": [
    "from src.data import twint_tools as tt\n",
    "# define keyword arguments\n",
    "kwargs = {'date_range':('2020-08-01 00:00:00', None),\n",
    "         'n_retries':5,\n",
    "         'suppress':False}\n",
    "# multi threading\n",
    "tt.twint_in_queue(tt._search_tweets_by_user, 3, journo_handles, args=('../data/raw/'+keyword+'_',), kwargs=kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Joined all the individual csv into one dataframe\n",
    "cyber_test = tt.join_tweet_csv(journo_handles, keyword)\n",
    "# Check\n",
    "cyber_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#this comes later\n",
    "\n",
    "\n",
    "# Standardise the twint output \n",
    "from src.data import data_cleanup as dc\n",
    "#Create the standardized template\n",
    "test_twint = dc.standard_tweet_dataset_setup('test_twint')\n",
    "test_twint\n",
    "#fill the template\n",
    "standard_tweet_twint = dc.fill_standard_tweet_dataset_with_twint(test_twint, cyber_test)\n",
    "# Check\n",
    "standard_tweet_twint.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the dataframe\n",
    "standard_tweet_twint.to_csv('../data/processed/' + keyword + '_standard_tweets_twint.csv',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Section 3.2: Extract mentions from Twint dataset<h4> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>mentions</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1295467814304849920</td>\n",
       "      <td>journalists4bel</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1295463883759648772</td>\n",
       "      <td>filleboheme</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1295463883759648772</td>\n",
       "      <td>terryandrob</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1294963552902680578</td>\n",
       "      <td>jonnelledge</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1293479566120570885</td>\n",
       "      <td>ukandeu</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              tweet_id         mentions\n",
       "0  1295467814304849920  journalists4bel\n",
       "1  1295463883759648772      filleboheme\n",
       "2  1295463883759648772      terryandrob\n",
       "3  1294963552902680578      jonnelledge\n",
       "4  1293479566120570885          ukandeu"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# from the twint dataset, extract mentions based on tweet id and save in a separate csv\n",
    "mentions_twint  = dc.twint_mentions_to_df(cyber_test)\n",
    "# Check\n",
    "mentions_twint.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the dataframe\n",
    "mentions_twint.to_csv('../data/processed/' + keyword + '_mentions_twint.csv',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Section 2.3: Scrape list of friends for each journalist using Twint  ############# we moved this up the pipeline<h4> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.data import twint_tools as tt\n",
    "# define keyword arguments\n",
    "kwargs = {#'date_range':('2020-08-01 00:00:00', None),\n",
    "         'n_retries':5,\n",
    "         'suppress':False}\n",
    "# multi threading\n",
    "tt.twint_in_queue(tt._get_friends, 3, journo_handles, args=('../data/raw/'+keyword+'_',), kwargs=kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenate all the individual lists into one dataframe with journalist and its friends\n",
    "friends_csv = tt.join_friends_csv(journo_handles,keyword) # this function has a bug, the first friend name is 'username'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the dataframe\n",
    "friends_csv.to_csv('../data/raw/'+keyword+'_journalist_friends.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Section 2.4: Scrape journalist user information using Twitter API ################ this also was moved up the pipeline<h4> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import tweepy\n",
    "from src.data import api_tweepy as api\n",
    "#Load twitter API credentials and return a tweepy API instance\n",
    "tw_api = api.connect_API('../src/data/twitter_credentials.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scrape user information using the API\n",
    "from src.data import api_user_tools as api_tools\n",
    "from src.data import data_cleanup as dc\n",
    "api_users = api_tools.batch_request_user_info(tw_api,journo_handles)\n",
    "df_api = dc.populate_user_df(api_users)\n",
    "# Check\n",
    "df_api.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the dataframe\n",
    "df_api.to_csv('../data/raw/'+keyword+'_user_profiles.csv', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Section 4. Loop over selected journalists handles and scrape their tweets (4.1) and mentions (4.2) using Twitter API<h3>\n",
    "    <h4>Section 4.1: Scrape tweets using Twint ################ I am waiting for Rob function here<h4> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import tweepy\n",
    "from src.data import api_tweepy as api\n",
    "#Load twitter API credentials and return a tweepy API instance\n",
    "tw_api = api.connect_API('../src/data/twitter_credentials.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Section 3.2: Extract mentions from API tweets<h4> \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Section 3.3: Scrape list of friends for each journalist using Twint ################### this has been moved up<h4>    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.data import twint_tools as tt\n",
    "# define keyword arguments\n",
    "kwargs = {#'date_range':('2020-08-01 00:00:00', None),\n",
    "         'n_retries':5,\n",
    "         'suppress':False}\n",
    "# multi threading\n",
    "tt.twint_in_queue(tt._get_friends, 3, journo_handles, args=('../data/raw/'+keyword+'_',), kwargs=kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenate all the individual lists into one dataframe with journalist and its friends\n",
    "friends_csv = tt.join_friends_csv(journo_handles,keyword) # this function has a bug, the first friend name is 'username'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the dataframe\n",
    "friends_csv.to_csv('../data/raw/'+keyword+'_journalist_friends.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Section 3.4: Scrape journalist user information using Twitter API ################### this has been moved u<h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import tweepy\n",
    "from src.data import api_tweepy as api\n",
    "#Load twitter API credentials and return a tweepy API instance\n",
    "tw_api = api.connect_API('../src/data/twitter_credentials.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scrape user information using the API\n",
    "from src.data import api_user_tools as api_tools\n",
    "from src.data import data_cleanup as dc\n",
    "api_users = api_tools.batch_request_user_info(tw_api,journo_handles)\n",
    "df_api = dc.populate_user_df(api_users)\n",
    "# Check\n",
    "df_api.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the dataframe\n",
    "df_api.to_csv('../data/raw/'+keyword+'_user_profiles.csv', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Section 5. data cleaning and standardization/LDA<h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Section 6. Create graph database and import twitter data into it<h3>\n",
    "    <h4>Section 6.1: Download modules, import modules and load graph database<h4> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install py2neo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import standard libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from py2neo import Graph\n",
    "from py2neo.data import Node, Relationship\n",
    "from src.data import graphdb\n",
    "\n",
    "# load / declare the database\n",
    "graph = graphdb.get_graph(new_graph = True)\n",
    "#graph = Graph(\"bolt://localhost:7687\", user=\"neo4j\", password=\"tweetoftheday\")\n",
    "#graph.begin()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LOAD the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Section 4.2: Load tweet data into import directory<h4> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# start with an empty graph, obviously don't run this if you already have stuff in there you don't want to delete\n",
    "graph.delete_all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First we want to load in our tweet information, to do this we need to put the file containing the tweets in the location/import directory\n",
    "!mv ~/Downloads/standardised_cyber_tweets.csv import/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Section 4.3. Create nodes representing tweets and users<h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Below, we use three cypher commands. The first loads the file. The second creates nodes representing tweets. The third creates nodes representing people. \n",
    "#Note that \"CREATE\" and \"MERGE\" are slightly different. \"CREATE\" makes a new node but if that node already exists then it does nothing. \n",
    "#\"MERGE\" creates a node if it doesn't already exist, and if it does exist will add or update information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load in tweets and twitter user information\n",
    "query_string = '''\n",
    "   LOAD CSV WITH HEADERS FROM \"file:///standardised_cyber_tweets.csv\" AS row\n",
    "   \n",
    "   CREATE (t:Tweet {tweet_id: row.tweet_id, conversation_id: row.conversation_id, user_id: row.user_id, \n",
    "   reply_to: row.reply_to, tweet_created_at_date: row.tweet_created_at_date, \n",
    "   tweet_created_at_time: row.tweet_created_at_time, text: row.text, replies_count: row.replies_count, \n",
    "   retweets_count: row.reteets_count, favourite_count: row.favourite_count, likes_count: row.likes_count,\n",
    "   hashtags: row.hashtags, topics: row.topics})\n",
    "   \n",
    "   MERGE (p:Person {user_id: row.user_id, screen_name: row.screen_name, name: row.name, \n",
    "   user_description: row.user_description, user_friends_n: row.user_friends_n, user_followers_n: row.user_followers_n, \n",
    "   prof_created_at: row.prof_created_at, favourites_count: row.favourites_count, verified: row.verified, \n",
    "   statuses_count: row.statuses_count});\n",
    "   '''\n",
    "# run cypher query\n",
    "graph.run(query_string)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
