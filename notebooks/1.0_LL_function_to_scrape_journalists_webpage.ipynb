{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"1.0_LL_function_to_scrape_journalists_webpage.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyNuABM5VWQ55kjDy/NmXGib"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"O1X3dWi0oEKQ","colab_type":"text"},"source":["## **Function for scraping twitter handles of journalists based on a specific keyword**\n","Date 06/08/2020"]},{"cell_type":"code","metadata":{"id":"57NUlz13_w3G","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1596698355302,"user_tz":-120,"elapsed":704,"user":{"displayName":"luca lamoni","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiZMpGjRPw1MibJF_PsAz5kbsPgPkSGpE7n4w8mKQ=s64","userId":"07353226337550152050"}}},"source":["# import libraries\n","# Note we need to check library version\n","import pandas as pd\n","from urllib.request import urlopen\n","from bs4 import BeautifulSoup\n","import re"],"execution_count":1,"outputs":[]},{"cell_type":"code","metadata":{"id":"812HEzr5AE8t","colab_type":"code","colab":{}},"source":["def scrape_journalist_website(keyword):\n","  \"\"\"\n","  This function scrapes a specific web page that contains information about journalists based in the UK\n","  who have a presence on Twitter. The fuction takes 1 argument which is the keyword that help us filter the journalists\n","  based on their field of expertise\n","  \"\"\"\n","  # Set the url search=keyword\n","  url = r\"https://www.journalism.co.uk/prof/?search=\" + keyword + \"&chunk=0&cmd=default\"\n","  html = urlopen(url)\n","\n","  # create two empty lists to store results\n","  profile_handles_page = []\n","  twitter_profile_names = []\n","  \n","  # create the loop that goes through the number of pages\n","  check = False\n","  i = 0\n","  while not check:\n","    # Define specific page url\n","    page_url = r\"https://www.journalism.co.uk/prof/?search=\" + keyword + \"&chunk=\" + str(i) + \"&cmd=default\"\n","    html = urlopen(page_url)\n","\n","    # Parse the webpage \n","    soup = BeautifulSoup(html, 'html.parser')\n","    \n","    # Create an empty list and fill it with a specific sub set of the web page that contains journalists' information\n","    temp_content_list = []\n","    for x in soup.find_all('div', class_=\"holder\"):\n","      temp_content_list.append(x)\n","      \n","      # Find within the subsett all twitter handles\n","      profile_handles_page = re.findall(r'@(\\w+)', str(temp_content_list))\n","\n","    # Each keyword will produce different number of journalists (i.e. differnt number of pages)\n","    # Since we are not sure how many pages we need to scrap (if we set a high fixed number beforehand, we are redirected to the last page of the search)\n","    # So to avoid repetions of twitter handles we check if any of the twittr handles we are appending is already present in twitter_profile_names\n","    # if it is we break the loop and return our list as a Pandas dataframe\n","    check =  any(item in profile_handles_page for item in twitter_profile_names)\n","    if check is False:\n","      for twitter_handle in profile_handles_page:\n","        twitter_profile_names.append(twitter_handle)\n","\n","    i += 1\n","\n","  \n","  return pd.DataFrame(twitter_profile_names, columns = ['twitter_handle'])\n"],"execution_count":null,"outputs":[]}]}